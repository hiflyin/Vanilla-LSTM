{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################################### sample 0 \n",
      " Training sample: 114 + 33 = 147\n",
      " Result is 255.0\n",
      " Average binarry error for this batch is [ 0.00062817]\n",
      "#################################################################################################### sample 1000 \n",
      " Training sample: 29 + 111 = 140\n",
      " Result is 8.0\n",
      " Average binarry error for this batch is [ 0.54830121]\n",
      "#################################################################################################### sample 2000 \n",
      " Training sample: 50 + 105 = 155\n",
      " Result is 27.0\n",
      " Average binarry error for this batch is [ 0.46295282]\n",
      "#################################################################################################### sample 3000 \n",
      " Training sample: 38 + 86 = 124\n",
      " Result is 124.0\n",
      " Average binarry error for this batch is [ 0.34680542]\n",
      "#################################################################################################### sample 4000 \n",
      " Training sample: 121 + 35 = 156\n",
      " Result is 156.0\n",
      " Average binarry error for this batch is [ 0.25683188]\n",
      "#################################################################################################### sample 5000 \n",
      " Training sample: 71 + 81 = 152\n",
      " Result is 156.0\n",
      " Average binarry error for this batch is [ 0.20295929]\n",
      "#################################################################################################### sample 6000 \n",
      " Training sample: 91 + 108 = 199\n",
      " Result is 207.0\n",
      " Average binarry error for this batch is [ 0.15409827]\n",
      "#################################################################################################### sample 7000 \n",
      " Training sample: 13 + 24 = 37\n",
      " Result is 37.0\n",
      " Average binarry error for this batch is [ 0.13368703]\n",
      "#################################################################################################### sample 8000 \n",
      " Training sample: 47 + 32 = 79\n",
      " Result is 79.0\n",
      " Average binarry error for this batch is [ 0.11152648]\n",
      "#################################################################################################### sample 9000 \n",
      " Training sample: 12 + 12 = 24\n",
      " Result is 24.0\n",
      " Average binarry error for this batch is [ 0.11806165]\n",
      "#################################################################################################### sample 10000 \n",
      " Training sample: 115 + 88 = 203\n",
      " Result is 203.0\n",
      " Average binarry error for this batch is [ 0.11523606]\n",
      "#################################################################################################### sample 11000 \n",
      " Training sample: 108 + 35 = 143\n",
      " Result is 131.0\n",
      " Average binarry error for this batch is [ 0.12693505]\n",
      "#################################################################################################### sample 12000 \n",
      " Training sample: 4 + 96 = 100\n",
      " Result is 100.0\n",
      " Average binarry error for this batch is [ 0.12994383]\n",
      "#################################################################################################### sample 13000 \n",
      " Training sample: 89 + 5 = 94\n",
      " Result is 94.0\n",
      " Average binarry error for this batch is [ 0.10065884]\n",
      "#################################################################################################### sample 14000 \n",
      " Training sample: 77 + 65 = 142\n",
      " Result is 142.0\n",
      " Average binarry error for this batch is [ 0.07971271]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Simple RNN for adding 2 numbers in binary\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "n_samples = 15000\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "n_bit = 8\n",
    "hidden_size = 16\n",
    "learning_rate = .1\n",
    "\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "weights_hidden = np.random.standard_normal(size=(input_dim, hidden_size))\n",
    "weights_previous_hidden = np.random.standard_normal(size=(hidden_size, hidden_size))\n",
    "weights_output = np.random.standard_normal(size=(hidden_size, output_dim))\n",
    "\n",
    "def sigmoid(x): return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "\n",
    "batch_error = 0\n",
    "\n",
    "# online learning: network gets updated with each sample on the way\n",
    "for i in range(n_samples):\n",
    "\n",
    "    # generate 2 random numbers and their sum\n",
    "    input_1, input_2 = np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number)\n",
    "    true_output = input_1 + input_2\n",
    "    \n",
    "    # calculate the binaries\n",
    "    input_1_binary, input_2_binary, true_output_binary = [int(x) for x in np.binary_repr(input_1, n_bit)], [int(x) for x\n",
    "                                in np.binary_repr(input_2, n_bit)], [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "\n",
    "    # we'll append the outputs at each layer on the way..\n",
    "    hidden_layer_output_seq = []\n",
    "    hidden_layer_output_seq.append(np.zeros((1,hidden_size)))\n",
    "    output_layer_output_seq = []\n",
    "\n",
    "    # forward pass of the bit sequence through the network and accumulating the errors at each bit position\n",
    "    for bit_idx in range(n_bit - 1, -1, -1):\n",
    "        \n",
    "        input_bits = np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]])\n",
    "        hidden_layer_outputs = sigmoid(np.dot(input_bits, weights_hidden) + np.dot(hidden_layer_output_seq[-1], weights_previous_hidden))\n",
    "        output_layer_output = sigmoid(np.dot(hidden_layer_outputs, weights_output))\n",
    "        \n",
    "\n",
    "        hidden_layer_output_seq.append(copy.deepcopy(hidden_layer_outputs))\n",
    "        output_layer_output_seq.append(copy.deepcopy(output_layer_output))\n",
    "        \n",
    "    \n",
    "    previous_hidden_layer_error_weighted_derivative = np.zeros((1,hidden_size))\n",
    "    # append one more zero array for going backwards\n",
    "    \n",
    "    # sum of the derivative of the outputs at the corresponding layers weighted by the errors, for each pair of input bits\n",
    "    sum_hidden_layer_updates = np.zeros_like(weights_hidden)\n",
    "    sum_previous_hidden_layer_updates = np.zeros_like(weights_previous_hidden)\n",
    "    sum_output_layer_updates = np.zeros_like(weights_output)\n",
    "\n",
    "    # rolling back from the last bit to the first\n",
    "    #hidden_layer_output_seq = hidden_layer_output_seq[1:]\n",
    "    hidden_layer_output_seq.reverse()\n",
    "    output_layer_output_seq.reverse()\n",
    "    \n",
    "    for bit_idx in range(n_bit):\n",
    "               \n",
    "        # take output error at this position -> size(output_dim)\n",
    "        output_error = np.array([true_output_binary[bit_idx]]) - output_layer_output_seq[bit_idx]\n",
    "\n",
    "        # calculate output derivative weighted by the output errors -> size(output_dim)\n",
    "        output_error_weighted_derivative = sigmoid_derivative(output_layer_output_seq[bit_idx])* output_error\n",
    "        \n",
    "        # sum the output_error_weighted_derivative for each element in the sequence weighted by the size of inputs int this layer -> (hidden_size, output_dim)\n",
    "        sum_output_layer_updates += np.dot(hidden_layer_output_seq[bit_idx].T, output_error_weighted_derivative)\n",
    "\n",
    "        # calculate hidden error as coming from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "        #  -> (output_dim)* (hidden_size, output_dim) + (hidden_size)*(hidden_size, hidden_size) = (hidden_size)\n",
    "        hidden_error = np.dot(output_error_weighted_derivative, weights_output.T) + np.dot(previous_hidden_layer_error_weighted_derivative, weights_previous_hidden)\n",
    "\n",
    "        # calculate hidden outputs derivatives weighted by hidden errors ->(hidden_size) * (hidden_size) = (hidden_size)\n",
    "        # print hidden_layer_output_seq[bit_idx].T.shape\n",
    "        # print hidden_error.shape\n",
    "        hidden_error_weighted_derivative = sigmoid_derivative(hidden_layer_output_seq[bit_idx])* hidden_error\n",
    "        # print hidden_error_weighted_derivative.shape\n",
    "        \n",
    "        # sum the output_error_weighted_derivative for each element in the sequence, weighted by the size of the inputs -> (input_dim, hidden_size)\n",
    "        sum_hidden_layer_updates += np.dot(np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]]).T, hidden_error_weighted_derivative)\n",
    "\n",
    "        # sum the hidden_error_weighted_derivative for each element in the sequence, weighted by the size of the inputs -> (hidden_size, hidden_size)\n",
    "        sum_previous_hidden_layer_updates += np.dot(hidden_layer_output_seq[bit_idx + 1].T, hidden_error_weighted_derivative)\n",
    "        \n",
    "        # propagating the hidden layer error back to\n",
    "        previous_hidden_layer_error_weighted_derivative = hidden_error_weighted_derivative\n",
    "        \n",
    "        # just accumulating error for printing\n",
    "        batch_error += abs(output_error[0])\n",
    "\n",
    "    # updating weights for this sample\n",
    "    weights_hidden += (sum_hidden_layer_updates * learning_rate)\n",
    "    weights_previous_hidden += (sum_previous_hidden_layer_updates * learning_rate)\n",
    "    weights_output += (sum_output_layer_updates * learning_rate)\n",
    "    \n",
    "    errors = np.array(true_output_binary) - np.array([x.tolist()[0][0] for x in output_layer_output_seq])\n",
    "    batch_error += sum([abs(x) for x in errors])/n_bit\n",
    "    \n",
    "    if (i % 1000) == 0: \n",
    "        print 100*'#' + \" sample {} \".format(i)\n",
    "        print \" Training sample: {0} + {1} = {2}\".format(input_1, input_2, true_output)\n",
    "        #print \" Binary version: {0} + {1} = {2}\".format(input_1_binary, input_2_binary, true_output_binary)\n",
    "        result = [x.tolist()[0][0] for x in output_layer_output_seq]\n",
    "        print \" Result is {}\".format( sum([pow(2,n_bit-i-1)*round(result[i]) for i in range(n_bit)]))\n",
    "        #print result\n",
    "        \n",
    "        print \" Average binarry error for this batch is {}\".format(batch_error/8000)   \n",
    "        batch_error = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
