{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple RNN for adding 2 numbers in binary\n",
    "\n",
    "forward forward..\n",
    "backward - we want to iteratively change the weights starting from random positions \n",
    "to minimize error we change the weights of each layer in the directions of the derivatives of the output  of that layer\n",
    "(note that the overall error function to optimize is formed by all layer functions but as we go back we no longer care  \n",
    "about the functions in front and their weights..)\n",
    "we want the change to be proportional to the size of the error and also the size of the input - so we weight the derivatives \n",
    "by the errors deltas and inputs\n",
    "when passing error delta back to previous layer - we multiply current error weighted derivative by the weights to see how much\n",
    "of the erro corresponds to each of the previous layer outputs\n",
    "'''\n",
    "\n",
    "\n",
    "######################################### THE DATA ######################################### \n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "np.random.seed(0)\n",
    "from numpy import ones, zeros\n",
    "\n",
    "# the data generating params\n",
    "n_samples = 1#5000\n",
    "n_bit = 8\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "\n",
    "#### done with constants\n",
    "\n",
    "def generate_random_sample():\n",
    "    # generate 2 random numbers and their sum\n",
    "    input_1, input_2 = np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number)\n",
    "    true_output = input_1 + input_2\n",
    "\n",
    "    # calculate the binaries\n",
    "    input_1_binary = [int(x) for x in np.binary_repr(input_1, n_bit)]\n",
    "    input_2_binary = [int(x) for x in np.binary_repr (input_2, n_bit)]\n",
    "    true_output_binary = [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "    \n",
    "    return input_1_binary, input_2_binary, true_output_binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################# THE RNN #############################################\n",
    "\n",
    "# RNN params\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "recursive_size = 16\n",
    "learning_rate = .1\n",
    "\n",
    "# RNN weights\n",
    "# simple RNN with one recurent hidden layer and one output layer\n",
    "\n",
    "# hidden layer weights\n",
    "w_recursive = np.random.standard_normal(size=(input_dim, recursive_size))\n",
    "w_previous_recursive = np.random.standard_normal(size=(recursive_size, recursive_size))\n",
    "# output layer weights\n",
    "w_dense = np.random.standard_normal(size=(recursive_size, output_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN Functions\n",
    "\n",
    "# util math functions\n",
    "def sigmoid(x): return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "\n",
    "def feed_forward_recursive_layer(inputs):#input_data, previous_recursive_layer_output):\n",
    "\n",
    "    raw_outputs = np.dot(inputs[\"from_previous\"], w_recursive) + z, w_previous_recursive)\n",
    "\n",
    "    return {\"raw\": raw_outputs, \"activation\": sigmoid(raw_outputs)}\n",
    "\n",
    "# backprop through time rnn layer \n",
    "# takes: its raw output, all the errors deltas sent to its successors\n",
    "# returns: the overall error delta to pass to its precedessors and the deltas to update its own weights\n",
    "def backprop_recursive_layer(inputs, outputs, errors):#error_to_output, error_to_next_recursive,  layer_raw_output):\n",
    "    \n",
    "    # calculate error as coming back from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "    error = np.dot(errors[\"to_output\"], w_dense) + np.dot(errors[\"to_next_recursive\"], w_previous_recursive)\n",
    "    # total delta of the layer to pass further down to previous inputing layers: error_weighted_derivative of output\n",
    "    total_delta = sigmoid_derivative(outputs[\"raw\"])* error \n",
    "    # delta corresponding to input from below layer based on inputs from that layer\n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta) \n",
    "    # delta corresponding to input from previous hidden layer based on inputs from that layer\n",
    "    recursive_w_delta = np.dot(inputs[\"from_recursive\"].T, total_delta)\n",
    "    \n",
    "    return {\"total_delta\": total_delta, \"recursive_w_delta\" : recursive_w_delta, \"input_w_delta\" : input_w_delta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "def feed_forward_dense_layer(inputs):\n",
    "    \n",
    "    raw_output = np.dot(inputs[\"from_previous\"], w_dense)\n",
    "    \n",
    "    return {\"raw\": raw_output, \"activation\": sigmoid(raw_output)}\n",
    "\n",
    "# gets the error delta it sent to output and the layer input and returns the delta to pass down and \n",
    "# the delta to update its weights\n",
    "def backprop_dense_layer(inputs, outputs, errors):\n",
    "    \n",
    "    # delta at this layer\n",
    "    total_delta = 1* errors[\"to_output\"] # being the output dense layer, derivative = 1\n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta)\n",
    "    \n",
    "    return {\"total_delta\": total_delta, \"input_w_delta\" : input_w_delta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# feed forward one sample unit through all layers\n",
    "def feed_forward_network(inputs):\n",
    "    \n",
    "    recursive_layer_outputs = feed_forward_recursive_layer(inputs)\n",
    "    dense_layer_outputs = feed_forward_dense_layer({\"from_previous\": recursive_layer_outputs[\"activation\"]})\n",
    "    \n",
    "    return {\"from_dense\":dense_layer_outputs,\"from_recursive\":recursive_layer_outputs}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# back prop one sample unit through all layers\n",
    "# because it's recursive it takes possible deltas from successor samples feeded forward, just as the feed forward takes recursive \n",
    "# outputs from previous samples \n",
    "# should return/fill the updates coresponding to this sample\n",
    "def back_prop_network(inputs, all_layer_outputs, correct_output, next_sample_deltas):\n",
    "    \n",
    "    inputs_dense = {\"from_previous\":all_layer_outputs[\"from_recursive\"][\"activation\"]}\n",
    "    outputs_dense = all_layer_outputs[\"from_dense\"]\n",
    "    errors_dense = {\"to_output\": correct_output - all_layer_outputs[\"from_dense\"][\"activation\"]}\n",
    "    dense_deltas = backprop_dense_layer(inputs_dense, outputs_dense, errors_dense)\n",
    "    \n",
    "    inputs_recursive = inputs\n",
    "    outputs_recursive = all_layer_outputs[\"from_recursive\"]\n",
    "    errors_recursive = {\"to_output\": dense_deltas[\"total_delta\"],\n",
    "                       \"to_next_recursive\": next_sample_deltas[\"recursive_deltas\"][\"total_delta\"]} \n",
    "    recursive_deltas = backprop_recursive_layer(inputs_recursive, outputs_recursive, errors_recursive)\n",
    "    \n",
    "    return {\"dense_deltas\":dense_deltas, \"recursive_deltas\":recursive_deltas}  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 0, 1], [0, 0, 0], [0, 0, 1])\n",
      "[[ 0.5         0.73105858  0.88079708]]\n",
      "[[ 0  0 -2]]\n",
      "{'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}\n",
      "{'from_previous': array([[ 1.,  1.,  1.]])}\n",
      "{'raw': array([[ 3.]]), 'activation': array([[ 0.95257413]])}\n",
      "{'input_w_delta': array([[ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.]]), 'total_delta': 1}\n",
      "{'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.75,  0.75,  0.75]]), 'recursive_w_delta': array([[ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75]]), 'total_delta': array([[ 0.75,  0.75,  0.75]])}\n",
      "{'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.75,  0.75,  0.75]]), 'recursive_w_delta': array([[ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75]]), 'total_delta': array([[ 0.75,  0.75,  0.75]])}\n",
      "{'from_previous': array([[ 0.98201379,  0.98201379,  0.98201379]])}\n",
      "{'from_dense': {'raw': array([[ 2.94604137]]), 'activation': array([[ 0.95007606]])}, 'from_recursive': {'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (3,3) not aligned: 1 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-058499249405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mnext_sample_recursive_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"total_delta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecursive_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recursive_w_delta\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_w_delta\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mnext_sample_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"dense_deltas\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recursive_deltas\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnext_sample_recursive_deltas\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mback_prop_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_recursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_layer_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_sample_deltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-19d9a66827c4>\u001b[0m in \u001b[0;36mback_prop_network\u001b[0;34m(inputs, all_layer_outputs, correct_output, next_sample_deltas)\u001b[0m\n\u001b[1;32m     14\u001b[0m     errors_recursive = {\"to_output\": dense_deltas[\"total_delta\"],\n\u001b[1;32m     15\u001b[0m                        \"to_next_recursive\": next_sample_deltas[\"recursive_deltas\"][\"total_delta\"]} \n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mrecursive_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop_recursive_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_recursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_recursive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors_recursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"dense_deltas\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdense_deltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"recursive_deltas\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrecursive_deltas\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-4db7b6ad0371>\u001b[0m in \u001b[0;36mbackprop_recursive_layer\u001b[0;34m(inputs, outputs, errors)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtotal_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# delta corresponding to input from below layer based on inputs from that layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0minput_w_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from_previous\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# delta corresponding to input from previous hidden layer based on inputs from that layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mrecursive_w_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"from_recursive\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_delta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (3,3) not aligned: 1 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# test functions - not asserting correct results - just making sure they run with correct dimensions\n",
    "# set test constants\n",
    "n_bit = 3\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "recursive_size = 3\n",
    "sample_data= np.array([[0,1]])\n",
    "error = 1\n",
    "# init test weights to 1 for simple test of correct values\n",
    "w_recursive = np.ones((input_dim, recursive_size))\n",
    "w_previous_recursive = np.ones((recursive_size, recursive_size))\n",
    "w_dense = np.ones((recursive_size, output_dim))\n",
    "\n",
    "\n",
    "print generate_random_sample()\n",
    "print sigmoid(np.array([range(recursive_size)]))\n",
    "print sigmoid_derivative(np.array([range(recursive_size)]))\n",
    "\n",
    "inputs_recursive = {\"from_previous\": sample_data, \"from_recursive\": np.ones((1,recursive_size))}\n",
    "print feed_forward_recursive_layer(inputs_recursive)\n",
    "\n",
    "inputs_dense = {\"from_previous\": ones((1,recursive_size))}\n",
    "print feed_forward_dense_layer(inputs_dense) \n",
    "\n",
    "outputs_dense = {\"raw\": 0, \"activation\": 0}\n",
    "errors_dense = {\"to_output\": 1}\n",
    "print backprop_dense_layer(inputs_dense, outputs_dense, errors_dense) \n",
    "\n",
    "outputs_recursive = {\"raw\": ones((1,recursive_size))/2, \"activation\": ones((1,recursive_size))/2}\n",
    "# assume there was no error sent to next hidden layer\n",
    "errors_recursive_case1 = {\"to_output\": ones((1,recursive_size)), \"to_next_recursive\": zeros((1,recursive_size))}\n",
    "# assume there was no error sent to next layer (the output dense layer)\n",
    "errors_recursive_case2 = {\"to_output\": zeros((1,recursive_size)), \"to_next_recursive\": ones((1,recursive_size))}\n",
    "print backprop_recursive_layer(inputs_recursive,  outputs_recursive, errors_recursive_case1)\n",
    "print backprop_recursive_layer(inputs_recursive,  outputs_recursive, errors_recursive_case2)\n",
    "\n",
    "print feed_forward_network(inputs_recursive)\n",
    "all_layer_outputs = {\"from_dense\":outputs_dense, \"from_recursive\":outputs_recursive}\n",
    "correct_output = 1\n",
    "next_sample_recursive_deltas = {\"total_delta\": zeros((1,recursive_size)), \"recursive_w_delta\" : None, \"input_w_delta\" : None}\n",
    "next_sample_deltas = {\"dense_deltas\":None, \"recursive_deltas\":next_sample_recursive_deltas}\n",
    "print back_prop_network(inputs_recursive, all_layer_outputs, correct_output, next_sample_deltas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feeds forward a sequence of samples \n",
    "feed_forward_network_sequence()\n",
    "\n",
    "# back propagates a sequence of samples \n",
    "back_prop_network_sequence()\n",
    "\n",
    "# update weights\n",
    "update_network_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
