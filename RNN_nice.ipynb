{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple RNN for adding 2 numbers in binary\n",
    "\n",
    "forward forward..\n",
    "backward - we want to iteratively change the weights starting from random positions \n",
    "to minimize error we change the weights of each layer in the directions of the derivatives of the output  of that layer\n",
    "(note that the overall error function to optimize is formed by all layer functions but as we go back we no longer care  \n",
    "about the functions in front and their weights..)\n",
    "we want the change to be proportional to the size of the error and also the size of the input - so we weight the derivatives \n",
    "by the errors deltas and inputs\n",
    "when passing error delta back to previous layer - we multiply current error weighted derivative by the weights to see how much\n",
    "of the erro corresponds to each of the previous layer outputs\n",
    "'''\n",
    "\n",
    "\n",
    "######################################### THE DATA ######################################### \n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "np.random.seed(0)\n",
    "from numpy import ones, zeros, zeros_like, log, clip\n",
    "\n",
    "# the data generating params\n",
    "n_samples = 5000\n",
    "n_bit = 8\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "\n",
    "#### done with constants\n",
    "\n",
    "def generate_random_sample():\n",
    "    # generate 2 random numbers and their sum\n",
    "    input_1, input_2 = np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number)\n",
    "    true_output = input_1 + input_2\n",
    "\n",
    "    # calculate the binaries\n",
    "    input_1_binary = [int(x) for x in np.binary_repr(input_1, n_bit)]\n",
    "    input_2_binary = [int(x) for x in np.binary_repr (input_2, n_bit)]\n",
    "    true_output_binary = [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "    \n",
    "    return input_1_binary, input_2_binary, true_output_binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################# THE RNN #############################################\n",
    "\n",
    "# RNN params\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "recursive_size = 3\n",
    "learning_rate = .1\n",
    "\n",
    "# RNN weights\n",
    "# simple RNN with one recurent hidden layer and one output layer\n",
    "\n",
    "weights = { # hidden layer weights\n",
    "           \"recursive\": np.random.standard_normal(size=(input_dim, recursive_size)),  \n",
    "           \"previous_recursive\": np.random.standard_normal(size=(recursive_size, recursive_size)), \n",
    "           \"recurisve_bias\": zeros(recursive_size),\n",
    "            # output layer weights\n",
    "           \"dense\":np.random.standard_normal(size=(recursive_size, output_dim)),\n",
    "           \"dense_bias\": zeros(output_dim),\n",
    "            # the associated metrics with this set of weights' values\n",
    "            \"log_loss\":0\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN Functions\n",
    "\n",
    "# first thing first - what do we measure?\n",
    "def logloss(target, predicted, eps=1e-15): return log(1-clip(predicted, eps, 1-eps))*(target-1) - log(clip(predicted, eps, 1-eps))*target\n",
    "# compute the loss for a sequence of target and predicted values\n",
    "def compute_loss_seq(targets, predicted):\n",
    "    assert len(targets) == len(predicted)\n",
    "    return np.mean([logloss(x[0], x[1]) for x in np.stack([targets, predicted], 1)])\n",
    "\n",
    "# util math functions\n",
    "def sigmoid(x): return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "\n",
    "def feed_forward_recursive_layer(inputs, weights):#input_data, previous_recursive_layer_output):\n",
    "\n",
    "    raw_outputs = np.dot(inputs[\"from_previous\"], weights[\"recursive\"]) + np.dot(\n",
    "        inputs[\"from_recursive\"], weights[\"previous_recursive\"]) + weights[\"recursive_bias\"]\n",
    "\n",
    "    return {\"raw\": raw_outputs, \"activation\": sigmoid(raw_outputs)}\n",
    "\n",
    "# backprop through time rnn layer \n",
    "# takes: its raw output, all the errors deltas sent to its successors\n",
    "# returns: the overall error delta to pass to its precedessors and the deltas to update its own weights\n",
    "def backprop_recursive_layer(inputs, outputs, errors, weights):#error_to_output, error_to_next_recursive,  layer_raw_output):\n",
    "    \n",
    "    # calculate error as coming back from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "    error = np.dot(errors[\"to_output\"], weights[\"dense\"].T) + np.dot(errors[\"to_next_recursive\"], weights[\"previous_recursive\"])\n",
    "    # total delta of the layer to pass further down to previous inputing layers: error_weighted_derivative of output\n",
    "    total_delta = sigmoid_derivative(outputs[\"raw\"])* error \n",
    "    # delta corresponding to input from below layer based on inputs from that layer\n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta) \n",
    "    # delta corresponding to input from previous hidden layer based on inputs from that layer\n",
    "    recursive_w_delta = np.dot(inputs[\"from_recursive\"].T, total_delta)\n",
    "    \n",
    "    return {\"total_delta\": total_delta, \"recursive_w_delta\" : recursive_w_delta, \"input_w_delta\" : input_w_delta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "def feed_forward_dense_layer(inputs, weights):\n",
    "    \n",
    "    raw_output = np.dot(inputs[\"from_previous\"], weights[\"dense\"]) + weights[\"dense_bias\"]\n",
    "    \n",
    "    return {\"raw\": raw_output, \"activation\": sigmoid(raw_output)}\n",
    "\n",
    "# gets the error delta it sent to output and the layer input and returns the delta to pass down and \n",
    "# the delta to update its weights\n",
    "def backprop_dense_layer(inputs, outputs, errors, weights):\n",
    "    \n",
    "    # delta at this layer\n",
    "    total_delta = 1* errors[\"to_output\"] # being the output dense layer, derivative = 1\n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta)\n",
    "    \n",
    "    return {\"total_delta\": total_delta, \"input_w_delta\": input_w_delta }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# feed forward one sample unit through all layers\n",
    "def feed_forward_network(inputs, weights):\n",
    "    \n",
    "    recursive_layer_outputs = feed_forward_recursive_layer(inputs, weights)\n",
    "    dense_layer_outputs = feed_forward_dense_layer({\"from_previous\": recursive_layer_outputs[\"activation\"]}, weights)\n",
    "    \n",
    "    return {\"from_dense\":dense_layer_outputs,\"from_recursive\":recursive_layer_outputs}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# back prop one sample unit through all layers\n",
    "# because it's recursive it takes possible deltas from successor samples feeded forward, just as the feed forward takes recursive \n",
    "# outputs from previous samples \n",
    "# should return/fill the updates coresponding to this sample\n",
    "def back_prop_network(inputs, all_layer_outputs, target, next_sample_deltas, weights):\n",
    "    \n",
    "    inputs_dense = {\"from_previous\":all_layer_outputs[\"from_recursive\"][\"activation\"]}\n",
    "    outputs_dense = all_layer_outputs[\"from_dense\"]\n",
    "    errors_dense = {\"to_output\": target - all_layer_outputs[\"from_dense\"][\"activation\"]}\n",
    "    dense_deltas = backprop_dense_layer(inputs_dense, outputs_dense, errors_dense, weights)\n",
    "    \n",
    "    inputs_recursive = inputs\n",
    "    outputs_recursive = all_layer_outputs[\"from_recursive\"]\n",
    "    errors_recursive = {\"to_output\": dense_deltas[\"total_delta\"],\n",
    "                       \"to_next_recursive\": next_sample_deltas[\"recursive_deltas\"][\"total_delta\"]} \n",
    "    recursive_deltas = backprop_recursive_layer(inputs_recursive, outputs_recursive, errors_recursive, weights)\n",
    "    \n",
    "    return {\"dense_deltas\":dense_deltas, \"recursive_deltas\":recursive_deltas}  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# feeds forward a sequence of samples..\n",
    "def feed_forward_network_sequence(inputs_seq, weights):\n",
    "    \n",
    "    all_samples_output_seq =[]\n",
    "    for input_unit in inputs_seq:\n",
    "        all_samples_output_seq.append(feed_forward_network(input_unit, weights))\n",
    "   \n",
    "    return all_samples_output_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# back propagates a sequence of samples - we don't pass delta from previous sequence here\n",
    "def back_prop_network_sequence(inputs_seq, outputs_seq, target_seq, weights):\n",
    "    \n",
    "    # dense deltas are not going to be used so no init is needed\n",
    "    init_recursive_deltas = {\"total_delta\": zeros((1,recursive_size)), \n",
    "                             \"recursive_w_delta\" : zeros_like(w_previous_recursive), \"input_w_delta\" : zeros_like(w_recursive)}\n",
    "    init_dense_deltas = {\"total_delta\": 0,  \"input_w_delta\" : zeros_like(w_dense)}\n",
    "    all_deltas_seq = [{\"dense_deltas\":init_dense_deltas, \"recursive_deltas\":init_recursive_deltas}]\n",
    "    for i in range(1,len(inputs_seq)):\n",
    "        all_deltas_seq.append(back_prop_network(inputs_seq[-i], outputs_seq[-i], target_seq[-i], all_deltas_seq[-i], weights))\n",
    "        \n",
    "    # compute loos for the whole sequence\n",
    "    weights[\"log_loss\"] += compute_loss_seq(target_seq, outputs_seq)\n",
    "        \n",
    "    return all_deltas_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update weights with a seq  of deltas coresponding to a sequence of inputs\n",
    "# also compute the log loss of the previous set of weights\n",
    "def update_network_weights(all_deltas_seq, weights ):  \n",
    "    \n",
    "    for all_deltas in all_deltas_seq:\n",
    "        weights[\"recursive\"] -= learning_rate*np.clip(all_deltas[\"recursive_deltas\"][\"input_w_delta\"], -3, 3)\n",
    "        weights[\"previous_recursive\"] -= learning_rate*np.clip(all_deltas[\"recursive_deltas\"][\"recursive_w_delta\"], -3, 3)\n",
    "        weights[\"recursive_bias\"] -= learning_rate*np.clip(all_deltas[\"recursive_deltas\"][\"total_delta\"], -3, 3)\n",
    "        weights[\"dense\"] -= learning_rate*np.clip(all_deltas[\"dense_deltas\"][\"input_w_delta\"], -3, 3)\n",
    "        weights[\"dense_bias\"] -= learning_rate*np.clip(all_deltas[\"dense_deltas\"][\"total_delta\"], -3, 3)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(weights):\n",
    "    for i in range(n_samples):\n",
    "        \n",
    "        input_1_binary, input_2_binary, target_binary = generate_random_sample()\n",
    "        input_seq = [ {\"from_previous\": list(x), \"from_recursive\": zeros((1,recursive_size))} for x in zip(input_1_binary, input_2_binary)]\n",
    "        \n",
    "        update_network_weights(back_prop_network_sequence(inputs_seq, feed_forward_network_sequence(inputs_seq, weights), \n",
    "                                                        target_binary, weights), weights)\n",
    "\n",
    "        if True:#i % 10 ==0: \n",
    "            print \"--------------\"\n",
    "            print float(weights[\"log_loss\"])/(i+1)\n",
    "            print \"--------------\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 0], [0, 1, 0], [1, 0, 0])\n",
      "[[ 0.5         0.73105858  0.88079708]]\n",
      "[[ 0  0 -2]]\n",
      "{'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}\n",
      "{'raw': array([[ 3.]]), 'activation': array([[ 0.95257413]])}\n",
      "{'input_w_delta': array([[ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.]]), 'total_delta': 1}\n",
      "{'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.25,  0.25,  0.25]]), 'recursive_w_delta': array([[ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25]]), 'total_delta': array([[ 0.25,  0.25,  0.25]])}\n",
      "{'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.75,  0.75,  0.75]]), 'recursive_w_delta': array([[ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75]]), 'total_delta': array([[ 0.75,  0.75,  0.75]])}\n",
      "{'dense_deltas': {'input_w_delta': array([[ 0.5],\n",
      "       [ 0.5],\n",
      "       [ 0.5]]), 'total_delta': 1}, 'recursive_deltas': {'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.25,  0.25,  0.25]]), 'recursive_w_delta': array([[ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25]]), 'total_delta': array([[ 0.25,  0.25,  0.25]])}}\n",
      "[{'from_dense': {'raw': array([[ 2.94604137]]), 'activation': array([[ 0.95007606]])}, 'from_recursive': {'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}}, {'from_dense': {'raw': array([[ 2.94604137]]), 'activation': array([[ 0.95007606]])}, 'from_recursive': {'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}}]\n",
      "[{'dense_deltas': {'input_w_delta': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'total_delta': 0}, 'recursive_deltas': {'input_w_delta': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'recursive_w_delta': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'total_delta': array([[ 0.,  0.,  0.]])}}, {'dense_deltas': {'input_w_delta': array([[ 0.5],\n",
      "       [ 0.5],\n",
      "       [ 0.5]]), 'total_delta': 1}, 'recursive_deltas': {'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.25,  0.25,  0.25]]), 'recursive_w_delta': array([[ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25]]), 'total_delta': array([[ 0.25,  0.25,  0.25]])}}]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-9bec1966e7f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mupdate_network_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mback_prop_network_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mtrain_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-184-606590c057da>\u001b[0m in \u001b[0;36mtrain_net\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         update_network_weights(back_prop_network_sequence(inputs_seq, feed_forward_network_sequence(inputs_seq, weights), \n\u001b[0;32m----> 8\u001b[0;31m                                                         target_binary, weights), weights)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#i % 10 ==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-182-310b91af59fb>\u001b[0m in \u001b[0;36mback_prop_network_sequence\u001b[0;34m(inputs_seq, outputs_seq, target_seq, weights)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# compute loos for the whole sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcompute_loss_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_deltas_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-164-2a4d8758004e>\u001b[0m in \u001b[0;36mcompute_loss_seq\u001b[0;34m(targets, predicted)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# compute the loss for a sequence of target and predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test functions - not asserting correct results - just making sure they run with correct dimensions\n",
    "# set test constants\n",
    "n_bit = 3\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "recursive_size = 3\n",
    "sample_data= np.array([[0,1]])\n",
    "\n",
    "error = 1\n",
    "# init test weights to 1 for simple test of correct values\n",
    "weights = {\"recursive\": np.ones((input_dim, recursive_size)),\n",
    "           \"previous_recursive\": np.ones((recursive_size, recursive_size)), \n",
    "           \"recursive_bias\": zeros((1, recursive_size)),\n",
    "           \"dense\": np.ones((recursive_size, output_dim)),\n",
    "           \"dense_bias\": zeros((1,output_dim)),\n",
    "           \"log_loss\":0\n",
    "          }\n",
    "\n",
    "\n",
    "print generate_random_sample()\n",
    "print sigmoid(np.array([range(recursive_size)]))\n",
    "print sigmoid_derivative(np.array([range(recursive_size)]))\n",
    "\n",
    "inputs_recursive = {\"from_previous\": sample_data, \"from_recursive\": np.ones((1,recursive_size))}\n",
    "print feed_forward_recursive_layer(inputs_recursive, weights)\n",
    "\n",
    "inputs_dense = {\"from_previous\": ones((1,recursive_size))}\n",
    "print feed_forward_dense_layer(inputs_dense, weights) \n",
    "\n",
    "outputs_dense = {\"raw\": 0, \"activation\": 0}\n",
    "errors_dense = {\"to_output\": 1}\n",
    "print backprop_dense_layer(inputs_dense, outputs_dense, errors_dense, weights) \n",
    "\n",
    "outputs_recursive = {\"raw\": ones((1,recursive_size))/2, \"activation\": ones((1,recursive_size))/2}\n",
    "# assume there was no error sent to next hidden layer\n",
    "errors_recursive_case1 = {\"to_output\": 1, \"to_next_recursive\": zeros((1,recursive_size))}\n",
    "# assume there was no error sent to next layer (the output dense layer)\n",
    "errors_recursive_case2 = {\"to_output\": 0, \"to_next_recursive\": ones((1,recursive_size))}\n",
    "print backprop_recursive_layer(inputs_recursive,  outputs_recursive, errors_recursive_case1, weights)\n",
    "print backprop_recursive_layer(inputs_recursive,  outputs_recursive, errors_recursive_case2, weights)\n",
    "\n",
    "#print feed_forward_network(inputs_recursive)\n",
    "all_layer_outputs = {\"from_dense\":outputs_dense, \"from_recursive\":outputs_recursive}\n",
    "correct_output = 1\n",
    "next_sample_recursive_deltas = {\"total_delta\": zeros((1,recursive_size)), \"recursive_w_delta\" : None, \"input_w_delta\" : None}\n",
    "next_sample_deltas = {\"dense_deltas\":None, \"recursive_deltas\":next_sample_recursive_deltas}\n",
    "print back_prop_network(inputs_recursive, all_layer_outputs, correct_output, next_sample_deltas, weights)\n",
    "inputs_seq = [inputs_recursive, inputs_recursive]\n",
    "outputs_seq = [all_layer_outputs, all_layer_outputs]\n",
    "target_seq = [correct_output, correct_output ]\n",
    "print feed_forward_network_sequence(inputs_seq, weights)\n",
    "print back_prop_network_sequence(inputs_seq, outputs_seq, target_seq, weights)\n",
    "\n",
    "n_samples = 5\n",
    "update_network_weights(back_prop_network_sequence(inputs_seq, outputs_seq, target_seq, weights), weights)\n",
    "train_net(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
