{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple RNN for adding 2 numbers in binary\n",
    "\n",
    "forward forward..\n",
    "backward - we want to iteratively change the weights starting from random positions \n",
    "to minimize error we change the weights of each layer in the directions of the derivatives of the output  of that layer\n",
    "(note that the overall error function to optimize is formed by all layer functions but as we go back we no longer care  \n",
    "about the functions in front and their weights..)\n",
    "we want the change to be proportional to the size of the error and also the size of the input - so we weight the derivatives \n",
    "by the errors deltas and inputs\n",
    "when passing error delta back to previous layer - we multiply current error weighted derivative by the weights to see how much\n",
    "of the erro corresponds to each of the previous layer outputs\n",
    "'''\n",
    "\n",
    "\n",
    "######################################### THE DATA ######################################### \n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "np.random.seed(0)\n",
    "from numpy import ones, zeros, zeros_like\n",
    "\n",
    "# the data generating params\n",
    "n_samples = 1#5000\n",
    "n_bit = 8\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "\n",
    "#### done with constants\n",
    "\n",
    "def generate_random_sample():\n",
    "    # generate 2 random numbers and their sum\n",
    "    input_1, input_2 = np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number)\n",
    "    true_output = input_1 + input_2\n",
    "\n",
    "    # calculate the binaries\n",
    "    input_1_binary = [int(x) for x in np.binary_repr(input_1, n_bit)]\n",
    "    input_2_binary = [int(x) for x in np.binary_repr (input_2, n_bit)]\n",
    "    true_output_binary = [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "    \n",
    "    return input_1_binary, input_2_binary, true_output_binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################################# THE RNN #############################################\n",
    "\n",
    "# RNN params\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "recursive_size = 3\n",
    "learning_rate = .1\n",
    "\n",
    "# RNN weights\n",
    "# simple RNN with one recurent hidden layer and one output layer\n",
    "\n",
    "weights = { # hidden layer weights\n",
    "           \"recursive\": np.random.standard_normal(size=(input_dim, recursive_size)),  \n",
    "           \"previous_recursive\": np.random.standard_normal(size=(recursive_size, recursive_size)), \n",
    "           \"recurisve_bias\": zeros(recursive_size),\n",
    "            # output layer weights\n",
    "           \"dense\":np.random.standard_normal(size=(recursive_size, output_dim)),\n",
    "           \"dense_bias\": zeros(output_dim)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN Functions\n",
    "\n",
    "# util math functions\n",
    "def sigmoid(x): return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "def logloss(target, predicted, eps=1e-15):\n",
    "    p = np.clip(target, eps, 1 - eps)\n",
    "    if target == 1:\n",
    "        return -log(p)\n",
    "    return -log(1 - p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "\n",
    "def feed_forward_recursive_layer(inputs, weights):#input_data, previous_recursive_layer_output):\n",
    "\n",
    "    raw_outputs = np.dot(inputs[\"from_previous\"], weights[\"recursive\"]) + np.dot(\n",
    "        inputs[\"from_recursive\"], weights[\"previous_recursive\"]) + weights[\"recursive_bias\"]\n",
    "\n",
    "    return {\"raw\": raw_outputs, \"activation\": sigmoid(raw_outputs)}\n",
    "\n",
    "# backprop through time rnn layer \n",
    "# takes: its raw output, all the errors deltas sent to its successors\n",
    "# returns: the overall error delta to pass to its precedessors and the deltas to update its own weights\n",
    "def backprop_recursive_layer(inputs, outputs, errors, weights):#error_to_output, error_to_next_recursive,  layer_raw_output):\n",
    "    \n",
    "    # calculate error as coming back from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "    error = np.dot(errors[\"to_output\"], weights[\"dense\"].T) + np.dot(errors[\"to_next_recursive\"], weights[\"previous_recursive\"])\n",
    "    # total delta of the layer to pass further down to previous inputing layers: error_weighted_derivative of output\n",
    "    total_delta = sigmoid_derivative(outputs[\"raw\"])* error \n",
    "    # delta corresponding to input from below layer based on inputs from that layer\n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta) \n",
    "    # delta corresponding to input from previous hidden layer based on inputs from that layer\n",
    "    recursive_w_delta = np.dot(inputs[\"from_recursive\"].T, total_delta)\n",
    "    \n",
    "    return {\"total_delta\": total_delta, \"recursive_w_delta\" : recursive_w_delta, \"input_w_delta\" : input_w_delta}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "def feed_forward_dense_layer(inputs, weights):\n",
    "    \n",
    "    raw_output = np.dot(inputs[\"from_previous\"], weights[\"dense\"]) + weights[\"dense_bias\"]\n",
    "    \n",
    "    return {\"raw\": raw_output, \"activation\": sigmoid(raw_output)}\n",
    "\n",
    "# gets the error delta it sent to output and the layer input and returns the delta to pass down and \n",
    "# the delta to update its weights\n",
    "def backprop_dense_layer(inputs, outputs, errors, weights):\n",
    "    \n",
    "    # delta at this layer\n",
    "    total_delta = 1* errors[\"to_output\"] # being the output dense layer, derivative = 1\n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta)\n",
    "    \n",
    "    return {\"total_delta\": total_delta, \"input_w_delta\": input_w_delta }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# feed forward one sample unit through all layers\n",
    "def feed_forward_network(inputs, weights):\n",
    "    \n",
    "    recursive_layer_outputs = feed_forward_recursive_layer(inputs, weights)\n",
    "    dense_layer_outputs = feed_forward_dense_layer({\"from_previous\": recursive_layer_outputs[\"activation\"]}, weights)\n",
    "    \n",
    "    return {\"from_dense\":dense_layer_outputs,\"from_recursive\":recursive_layer_outputs}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# back prop one sample unit through all layers\n",
    "# because it's recursive it takes possible deltas from successor samples feeded forward, just as the feed forward takes recursive \n",
    "# outputs from previous samples \n",
    "# should return/fill the updates coresponding to this sample\n",
    "def back_prop_network(inputs, all_layer_outputs, target, next_sample_deltas, weights):\n",
    "    \n",
    "    inputs_dense = {\"from_previous\":all_layer_outputs[\"from_recursive\"][\"activation\"]}\n",
    "    outputs_dense = all_layer_outputs[\"from_dense\"]\n",
    "    errors_dense = {\"to_output\": target - all_layer_outputs[\"from_dense\"][\"activation\"]}\n",
    "    dense_deltas = backprop_dense_layer(inputs_dense, outputs_dense, errors_dense, weights)\n",
    "    \n",
    "    inputs_recursive = inputs\n",
    "    outputs_recursive = all_layer_outputs[\"from_recursive\"]\n",
    "    errors_recursive = {\"to_output\": dense_deltas[\"total_delta\"],\n",
    "                       \"to_next_recursive\": next_sample_deltas[\"recursive_deltas\"][\"total_delta\"]} \n",
    "    recursive_deltas = backprop_recursive_layer(inputs_recursive, outputs_recursive, errors_recursive, weights)\n",
    "    \n",
    "    return {\"dense_deltas\":dense_deltas, \"recursive_deltas\":recursive_deltas}  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# feeds forward a sequence of samples..\n",
    "def feed_forward_network_sequence(inputs_seq, weights):\n",
    "    \n",
    "    all_samples_output_seq =[]\n",
    "    for input_unit in inputs_seq:\n",
    "        all_samples_output_seq.append(feed_forward_network(input_unit, weights))\n",
    "    \n",
    "    return all_samples_output_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# back propagates a sequence of samples - we don't pass delta from previous sequence here\n",
    "def back_prop_network_sequence(inputs_seq, outputs_seq, target_seq, weights):\n",
    "    \n",
    "    # dense deltas are not going to be used so no init is needed\n",
    "    \n",
    "    init_recursive_deltas = {\"total_delta\": zeros((1,recursive_size)), \n",
    "                             \"recursive_w_delta\" : zeros_like(w_previous_recursive), \"input_w_delta\" : zeros_like(w_recursive)}\n",
    "    init_dense_deltas = {\"total_delta\": 0,  \"input_w_delta\" : zeros_like(w_dense)}\n",
    "    all_deltas_seq = [{\"dense_deltas\":init_dense_deltas, \"recursive_deltas\":init_recursive_deltas}]\n",
    "    for i in range(1,len(inputs_seq)):\n",
    "        all_deltas_seq.append(back_prop_network(inputs_seq[-i], outputs_seq[-i], target_seq[-i], all_deltas_seq[-i], weights))\n",
    "        \n",
    "    return all_deltas_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# update weights with a seq  of deltas coresponding to a sequence of inputs\n",
    "def update_network_weights(all_deltas_seq, weights ):  \n",
    "    \n",
    "    for all_deltas in all_deltas_seq:\n",
    "        weights[\"recursive\"] -= learning_rate*np.clip(all_deltas[\"recursive_deltas\"][\"input_w_delta\"], -3, 3)\n",
    "        weights[\"previous_recursive\"] -= learning_rate*np.clip(all_deltas[\"recursive_deltas\"][\"recursive_w_delta\"], -3, 3)\n",
    "        weights[\"recursive_bias\"] -= learning_rate*np.clip(all_deltas[\"recursive_deltas\"][\"total_delta\"], -3, 3)\n",
    "        weights[\"dense\"] -= learning_rate*np.clip(all_deltas[\"dense_deltas\"][\"input_w_delta\"], -3, 3)\n",
    "        weights[\"dense_bias\"] -= learning_rate*np.clip(all_deltas[\"dense_deltas\"][\"total_delta\"], -3, 3)\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(weights):\n",
    "    for i in range(n_samples):\n",
    "        input_1_binary, input_2_binary, target_binary = generate_random_sample()\n",
    "        input_seq = [ {\"from_previous\": list(x), \"from_recursive\": zeros((1,recursive_size))} for x in zip(input_1_binary, input_2_binary)]\n",
    "        update_network_weights(back_prop_network_sequence(inputs_seq, feed_forward_network_sequence(inputs_seq, weights), \n",
    "                                                          target_binary, weights), weights)\n",
    "        print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 0], [0, 1, 1], [1, 0, 1])\n",
      "[[ 0.5         0.73105858  0.88079708]]\n",
      "[[ 0  0 -2]]\n",
      "{'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}\n",
      "{'raw': array([[ 3.]]), 'activation': array([[ 0.95257413]])}\n",
      "{'input_w_delta': array([[ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.]]), 'total_delta': 1}\n",
      "{'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.25,  0.25,  0.25]]), 'recursive_w_delta': array([[ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25]]), 'total_delta': array([[ 0.25,  0.25,  0.25]])}\n",
      "{'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.75,  0.75,  0.75]]), 'recursive_w_delta': array([[ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75],\n",
      "       [ 0.75,  0.75,  0.75]]), 'total_delta': array([[ 0.75,  0.75,  0.75]])}\n",
      "{'dense_deltas': {'input_w_delta': array([[ 0.5],\n",
      "       [ 0.5],\n",
      "       [ 0.5]]), 'total_delta': 1}, 'recursive_deltas': {'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.25,  0.25,  0.25]]), 'recursive_w_delta': array([[ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25]]), 'total_delta': array([[ 0.25,  0.25,  0.25]])}}\n",
      "[{'from_dense': {'raw': array([[ 2.94604137]]), 'activation': array([[ 0.95007606]])}, 'from_recursive': {'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}}, {'from_dense': {'raw': array([[ 2.94604137]]), 'activation': array([[ 0.95007606]])}, 'from_recursive': {'raw': array([[ 4.,  4.,  4.]]), 'activation': array([[ 0.98201379,  0.98201379,  0.98201379]])}}]\n",
      "[{'dense_deltas': {'input_w_delta': array([[ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.]]), 'total_delta': 0}, 'recursive_deltas': {'input_w_delta': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'recursive_w_delta': array([[ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.]]), 'total_delta': array([[ 0.,  0.,  0.]])}}, {'dense_deltas': {'input_w_delta': array([[ 0.5],\n",
      "       [ 0.5],\n",
      "       [ 0.5]]), 'total_delta': 1}, 'recursive_deltas': {'input_w_delta': array([[ 0.  ,  0.  ,  0.  ],\n",
      "       [ 0.25,  0.25,  0.25]]), 'recursive_w_delta': array([[ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25],\n",
      "       [ 0.25,  0.25,  0.25]]), 'total_delta': array([[ 0.25,  0.25,  0.25]])}}]\n",
      "{'dense': array([[ 0.9437846],\n",
      "       [ 0.9437846],\n",
      "       [ 0.9437846]]), 'previous_recursive': array([[ 1.04214648,  1.04214648,  1.04214648],\n",
      "       [ 1.04214648,  1.04214648,  1.04214648],\n",
      "       [ 1.04214648,  1.04214648,  1.04214648]]), 'recursive': array([[ 1.        ,  1.        ,  1.        ],\n",
      "       [ 1.04214648,  1.04214648,  1.04214648]]), 'recursive_bias': array([[ 0.04214648,  0.04214648,  0.04214648]]), 'dense_bias': array([[-0.10634439]])}\n"
     ]
    }
   ],
   "source": [
    "# test functions - not asserting correct results - just making sure they run with correct dimensions\n",
    "# set test constants\n",
    "n_bit = 3\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "recursive_size = 3\n",
    "sample_data= np.array([[0,1]])\n",
    "\n",
    "error = 1\n",
    "# init test weights to 1 for simple test of correct values\n",
    "weights = {\"recursive\": np.ones((input_dim, recursive_size)),\n",
    "           \"previous_recursive\": np.ones((recursive_size, recursive_size)), \n",
    "           \"recursive_bias\": zeros((1, recursive_size)),\n",
    "           \"dense\": np.ones((recursive_size, output_dim)),\n",
    "           \"dense_bias\": zeros((1,output_dim))\n",
    "          }\n",
    "\n",
    "\n",
    "print generate_random_sample()\n",
    "print sigmoid(np.array([range(recursive_size)]))\n",
    "print sigmoid_derivative(np.array([range(recursive_size)]))\n",
    "\n",
    "inputs_recursive = {\"from_previous\": sample_data, \"from_recursive\": np.ones((1,recursive_size))}\n",
    "print feed_forward_recursive_layer(inputs_recursive, weights)\n",
    "\n",
    "inputs_dense = {\"from_previous\": ones((1,recursive_size))}\n",
    "print feed_forward_dense_layer(inputs_dense, weights) \n",
    "\n",
    "outputs_dense = {\"raw\": 0, \"activation\": 0}\n",
    "errors_dense = {\"to_output\": 1}\n",
    "print backprop_dense_layer(inputs_dense, outputs_dense, errors_dense, weights) \n",
    "\n",
    "outputs_recursive = {\"raw\": ones((1,recursive_size))/2, \"activation\": ones((1,recursive_size))/2}\n",
    "# assume there was no error sent to next hidden layer\n",
    "errors_recursive_case1 = {\"to_output\": 1, \"to_next_recursive\": zeros((1,recursive_size))}\n",
    "# assume there was no error sent to next layer (the output dense layer)\n",
    "errors_recursive_case2 = {\"to_output\": 0, \"to_next_recursive\": ones((1,recursive_size))}\n",
    "print backprop_recursive_layer(inputs_recursive,  outputs_recursive, errors_recursive_case1, weights)\n",
    "print backprop_recursive_layer(inputs_recursive,  outputs_recursive, errors_recursive_case2, weights)\n",
    "\n",
    "#print feed_forward_network(inputs_recursive)\n",
    "all_layer_outputs = {\"from_dense\":outputs_dense, \"from_recursive\":outputs_recursive}\n",
    "correct_output = 1\n",
    "next_sample_recursive_deltas = {\"total_delta\": zeros((1,recursive_size)), \"recursive_w_delta\" : None, \"input_w_delta\" : None}\n",
    "next_sample_deltas = {\"dense_deltas\":None, \"recursive_deltas\":next_sample_recursive_deltas}\n",
    "print back_prop_network(inputs_recursive, all_layer_outputs, correct_output, next_sample_deltas, weights)\n",
    "inputs_seq = [inputs_recursive, inputs_recursive]\n",
    "outputs_seq = [all_layer_outputs, all_layer_outputs]\n",
    "target_seq = [correct_output, correct_output ]\n",
    "print feed_forward_network_sequence(inputs_seq, weights)\n",
    "print back_prop_network_sequence(inputs_seq, outputs_seq, target_seq, weights)\n",
    "\n",
    "update_network_weights(back_prop_network_sequence(inputs_seq, outputs_seq, target_seq, weights), weights)\n",
    "train_net(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do:\n",
    "\n",
    "# 3 loss print\n",
    "# 4 nice dict print\n",
    "# 5 gradient check\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
