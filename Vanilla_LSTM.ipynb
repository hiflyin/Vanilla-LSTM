{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################################### sample 0 \n",
      " Training sample: 5 + 3 = 8\n",
      " Result is 1.0\n",
      " Average binarry error for this batch is [ 0.00031903]\n",
      "#################################################################################################### sample 1000 \n",
      " Training sample: 5 + 2 = 7\n",
      " Result is 3.0\n",
      " Average binarry error for this batch is [ 0.31124201]\n",
      "#################################################################################################### sample 2000 \n",
      " Training sample: 5 + 5 = 10\n",
      " Result is 0.0\n",
      " Average binarry error for this batch is [ 0.3066118]\n",
      "#################################################################################################### sample 3000 \n",
      " Training sample: 6 + 4 = 10\n",
      " Result is 6.0\n",
      " Average binarry error for this batch is [ 0.29437916]\n",
      "#################################################################################################### sample 4000 \n",
      " Training sample: 2 + 4 = 6\n",
      " Result is 4.0\n",
      " Average binarry error for this batch is [ 0.28763247]\n",
      "#################################################################################################### sample 5000 \n",
      " Training sample: 3 + 2 = 5\n",
      " Result is 3.0\n",
      " Average binarry error for this batch is [ 0.28559298]\n",
      "#################################################################################################### sample 6000 \n",
      " Training sample: 2 + 3 = 5\n",
      " Result is 7.0\n",
      " Average binarry error for this batch is [ 0.28400294]\n",
      "#################################################################################################### sample 7000 \n",
      " Training sample: 1 + 5 = 6\n",
      " Result is 2.0\n",
      " Average binarry error for this batch is [ 0.27648762]\n",
      "#################################################################################################### sample 8000 \n",
      " Training sample: 2 + 6 = 8\n",
      " Result is 5.0\n",
      " Average binarry error for this batch is [ 0.27499908]\n",
      "#################################################################################################### sample 9000 \n",
      " Training sample: 0 + 4 = 4\n",
      " Result is 0.0\n",
      " Average binarry error for this batch is [ 0.26348495]\n",
      "#################################################################################################### sample 10000 \n",
      " Training sample: 1 + 4 = 5\n",
      " Result is 1.0\n",
      " Average binarry error for this batch is [ 0.25306598]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Simple LSTM for adding 2 numbers in binary\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "n_samples = 10001\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "n_bit = 8\n",
    "hidden_size = 16\n",
    "learning_rate = .1\n",
    "\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "\n",
    "# weights input values -> decide what are the values calculated as a function of current inputs and outputs from previous hidden layer\n",
    "w_hidden_input = np.random.standard_normal(size=(input_dim, hidden_size))\n",
    "w_previous_lstm_cell_output = np.random.standard_normal(size=(hidden_size, hidden_size))\n",
    "\n",
    "# weights input gate -> decide what information is kept as a function of current inputs and outputs from previous hidden layer\n",
    "w_hidden_input_gate = np.random.standard_normal(size=(input_dim, hidden_size))\n",
    "w_previous_lstm_cell_input_gate = np.random.standard_normal(size=(hidden_size, hidden_size))\n",
    "\n",
    "# weights forget gate -> decide what information is not kept as a function of current inputs and outputs from previous hidden layer\n",
    "w_hidden_forget_gate = np.random.standard_normal(size=(input_dim, hidden_size))\n",
    "w_previous_lstm_cell_forget_gate = np.random.standard_normal(size=(hidden_size, hidden_size))\n",
    "\n",
    "# weights output gate -> decide what information is not kept as a function of current inputs and outputs from previous hidden layer\n",
    "w_lstm_cell_output_gate = np.random.standard_normal(size=(input_dim, hidden_size))\n",
    "w_previous_lstm_cell_output_gate = np.random.standard_normal(size=(hidden_size, hidden_size))\n",
    "\n",
    "#  weights output layer\n",
    "weights_output = np.random.standard_normal(size=(hidden_size, output_dim))\n",
    "\n",
    "def sigmoid(x): return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "def tanh(x): return (np.exp(x) - np.exp(-x))  / (np.exp(x) + np.exp(-x)) \n",
    "def tanh_derivative(x): return 1-tanh(x)^2\n",
    "\n",
    "\n",
    "batch_error = 0\n",
    "\n",
    "# online learning: network gets updated with each sample on the way\n",
    "for i in range(n_samples):\n",
    "\n",
    "    # generate 2 random numbers and their sum\n",
    "    input_1, input_2 = np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number)\n",
    "    true_output = input_1 + input_2\n",
    "    \n",
    "    # calculate the binaries\n",
    "    input_1_binary, input_2_binary, true_output_binary = [int(x) for x in np.binary_repr(input_1, n_bit)], [int(x) for x\n",
    "                                in np.binary_repr(input_2, n_bit)], [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "\n",
    "    # we'll append the outputs at each layer on the way..\n",
    "    lstm_cell_output_seq = []\n",
    "    hidden_layer_filtered_values_seq = []\n",
    "    output_layer_output_seq = []\n",
    "    \n",
    "    lstm_cell_output_seq.append(np.zeros((1,hidden_size)))\n",
    "    hidden_layer_filtered_values_seq.append(np.zeros((1,hidden_size)))\n",
    "\n",
    "    # forward pass of the bit sequence through the network and accumulating the errors at each bit position\n",
    "    for bit_idx in range(n_bit - 1, -1, -1):\n",
    "        \n",
    "        input_bits = np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]])\n",
    "        \n",
    "        # feed forward through LSTM cell\n",
    "        hidden_layer_raw_values = tanh(np.dot(input_bits, w_hidden_input) + np.dot(lstm_cell_output_seq[-1], w_previous_lstm_cell_output))\n",
    "        hidden_layer_input_gate = sigmoid(np.dot(input_bits, w_hidden_input_gate) + \n",
    "                                          np.dot(lstm_cell_output_seq[-1], w_previous_lstm_cell_input_gate)) \n",
    "        hidden_layer_forget_gate = sigmoid(np.dot(input_bits, w_hidden_forget_gate) + \n",
    "                                           np.dot(lstm_cell_output_seq[-1], w_previous_lstm_cell_forget_gate))\n",
    "        \n",
    "        hidden_layer_filtered_values = tanh(hidden_layer_raw_values*hidden_layer_input_gate + \n",
    "                                            hidden_layer_filtered_values_seq[-1]*w_hidden_forget_gate)  \n",
    "    \n",
    "        lstm_cell_output_gate = sigmoid(np.dot(input_bits, w_lstm_cell_output_gate) + \n",
    "                                        np.dot(lstm_cell_output_seq[-1], w_previous_lstm_cell_output_gate))\n",
    "           \n",
    "        lstm_cell_output = hidden_layer_filtered_values*lstm_cell_output_gate\n",
    "\n",
    "        hidden_layer_filtered_values_seq.append(copy.deepcopy(hidden_layer_filtered_values))\n",
    "        lstm_cell_output_seq.append(copy.deepcopy(lstm_cell_output))\n",
    "        \n",
    "        # feed forward through output layer\n",
    "        output_layer_output = sigmoid(np.dot(lstm_cell_output, weights_output))\n",
    "        output_layer_output_seq.append(copy.deepcopy(output_layer_output))\n",
    "        \n",
    "    \n",
    "    #previous_hidden_layer_error_weighted_derivative = np.zeros((1,hidden_size))\n",
    "    # append one more zero array for going backwards\n",
    "    \n",
    "    #sum of the derivative of the outputs at the corresponding layers weighted by the errors, for each pair of input bits\n",
    "    #sum_hidden_layer_updates = np.zeros_like(weights_hidden)\n",
    "    #sum_previous_hidden_layer_updates = np.zeros_like(weights_previous_hidden)\n",
    "    #sum_output_previous_hidden_layer_error_weighted_derivativelayer_updates = np.zeros_like(weights_output)\n",
    "\n",
    "    # rolling back from the last bit to the first\n",
    "    hidden_layer_filtered_values_seq.reverse()\n",
    "    output_layer_output_seq.reverse()\n",
    "    lstm_cell_output_seq.reverse()\n",
    "    \n",
    "    for bit_idx in range(n_bit):\n",
    "               \n",
    "        # take output error at this position -> size(output_dim)\n",
    "        output_error = np.array([true_output_binary[bit_idx]]) - output_layer_output_seq[bit_idx]\n",
    "\n",
    "        # calculate output derivative weighted by the output errors -> size(output_dim) or what we call delta\n",
    "        output_error_weighted_derivative = sigmoid_derivative(output_layer_output_seq[bit_idx])* output_error\n",
    "        \n",
    "        # sum the output_error_weighted_derivative for each element in the sequence, weighted by the size of the inputs \n",
    "            # size -> (input_dim, hidden_size)\n",
    "        sum_hidden_layer_updates += np.dot(np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]]).T, hidden_error_weighted_derivative)\n",
    "        \n",
    "        # propagate output layer error to lstm cell error - this error was sent to (and therefore coming back from):\n",
    "            # 1. output layer - or next layer\n",
    "            # 2. next hidden layer raw values\n",
    "            # 3. next hidden layer filters / decisions: input gate & forget gate\n",
    "            # 4. next lstm output gate decision\n",
    "        # no derivatites to compute for the lstm cell output as it is not parameterized by any weights\n",
    "        \n",
    "        lstm_cell_error = np.dot(output_error_weighted_derivative, weights_output.T) + \n",
    "                            np.dot(previous_hidden_raw_values_error_weighted_derivative, w_previous_lstm_cell_output) +\n",
    "                            np.dot(previous_hidden_input_gate_error_weighted_derivative, w_hidden_input_gate) +\n",
    "                            np.dot(previous_hidden_forget_gate_error_weighted_derivative, w_hidden_forget_gate) +\n",
    "                            np.dot(previous_lstm_cell_output_gate_error_weighted_derivative, w_lstm_cell_output_gate)\n",
    "                        \n",
    "\n",
    "        # propagate lstm cell error to next lstm output gate decision - this error was sent to: current lstm_cell_output only\n",
    "        lstm_cell_output_gate_error = lstm_cell_error * hidden_layer_filtered_values_seq[bit_idx] * sigmoid_derivative(x)\n",
    "            \n",
    "        \n",
    "        # propagate lstm cell error to next filtered hidden layer output - this error was sent to:\n",
    "        # 1. next filtered hidden layer output - when deciding what to forget out of it\n",
    "        # 2. current lstm_cell_output\n",
    "        \n",
    "        # propagate lstm cell error to next hidden layer filter gates - these errors were sent to current filtered hidden layer output\n",
    "        \n",
    "        # propagate lstm cell error + \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # sum the output_error_weighted_derivative for each element in the sequence weighted by the size of inputs into this layer \n",
    "        sum_output_layer_updates += np.dot(hidden_layer_output_seq[bit_idx].T, output_error_weighted_derivative)\n",
    "\n",
    "        # calculate hidden error as coming from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "        hidden_error = np.dot(output_error_weighted_derivative, weights_output.T) + \n",
    "            np.dot(previous_hidden_layer_error_weighted_derivative, weights_previous_hidden)\n",
    "\n",
    "        # calculate hidden outputs derivatives weighted by hidden errors ->(hidden_size) * (hidden_size) = (hidden_size)\n",
    "        hidden_error_weighted_derivative = sigmoid_derivative(hidden_layer_output_seq[bit_idx])* hidden_error\n",
    "\n",
    "        # sum the output_error_weighted_derivative for each element in the sequence, weighted by the size of the inputs -> (input_dim, hidden_size)\n",
    "        sum_hidden_layer_updates += np.dot(np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]]).T, hidden_error_weighted_derivative)\n",
    "\n",
    "        # sum the hidden_error_weighted_derivative for each element in the sequence, weighted by the size of the inputs -> (hidden_size, hidden_size)\n",
    "        sum_previous_hidden_layer_updates += np.dot(hidden_layer_output_seq[bit_idx - 1].T, hidden_error_weighted_derivative)\n",
    "        \n",
    "        # propagating the hidden layer error back to\n",
    "        previous_hidden_layer_error_weighted_derivative = hidden_error_weighted_derivative\n",
    "        \n",
    "        # just accumulating error for printing\n",
    "        batch_error += abs(output_error[0])\n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "    # updating weights for this sample\n",
    "    weights_hidden += (sum_hidden_layer_updates * learning_rate)\n",
    "    weights_previous_hidden += (sum_previous_hidden_layer_updates * learning_rate)\n",
    "    weights_output += (sum_output_layer_updates * learning_rate)\n",
    "    \n",
    "    errors = np.array(true_output_binary) - np.array([x.tolist()[0][0] for x in output_layer_output_seq])\n",
    "    batch_error += sum([abs(x) for x in errors])/n_bit\n",
    "    \n",
    "    if (i % 1000) == 0: \n",
    "        print 100*'#' + \" sample {} \".format(i)\n",
    "        print \" Training sample: {0} + {1} = {2}\".format(input_1, input_2, true_output)\n",
    "        #print \" Binary version: {0} + {1} = {2}\".format(input_1_binary, input_2_binary, true_output_binary)\n",
    "        result = [x.tolist()[0][0] for x in output_layer_output_seq]\n",
    "        print \" Result is {}\".format( sum([pow(2,n_bit-i-1)*round(result[i]) for i in range(n_bit)]))\n",
    "        #print result\n",
    "        \n",
    "        print \" Average binarry error for this batch is {}\".format(batch_error/8000)   \n",
    "        batch_error = 0\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
