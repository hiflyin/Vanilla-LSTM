{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 3)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "n_samples = 2\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "n_bit = 3\n",
    "hidden_size = 3\n",
    "learning_rate = .1\n",
    "\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "weights_hidden1 = np.random.standard_normal(size=(input_dim, hidden_size))\n",
    "weights_previous_hidden1 = np.random.standard_normal(size=(hidden_size, hidden_size))\n",
    "weights_output1 = np.random.standard_normal(size=(hidden_size, output_dim))\n",
    "\n",
    "samples = [(np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number))]\n",
    "samples.append((np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number)))\n",
    "samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################################### sample 0 \n",
      " Training sample: 1 + 1 = 2\n",
      "output layer: [[ 0.23025928]]\n",
      "output layer: [[ 0.24095597]]\n",
      "output layer: [[ 0.30072392]]\n",
      "[[ 0.00299557  0.00224778  0.0022617 ]]\n",
      "[[ 0.00068787  0.00074533  0.00018759]\n",
      " [ 0.00068787  0.00074533  0.00018759]]\n",
      "[[ -1.16328202e-03  -3.56168577e-04  -8.98100720e-04]\n",
      " [ -1.08546660e-03  -3.26680035e-04  -8.46291755e-04]\n",
      " [  9.61923361e-05   8.27670345e-06   1.05176880e-04]]\n",
      "#################################################################################################### sample 1 \n",
      " Training sample: 1 + 3 = 4\n",
      "output layer: [[ 0.2684822]]\n",
      "output layer: [[ 0.33647413]]\n",
      "output layer: [[ 0.30132626]]\n",
      "[[ 0.00176992  0.00042192  0.00222843]]\n",
      "[[ 0.00183742  0.00038005  0.00023555]\n",
      " [ 0.0025249   0.00109878  0.00039585]]\n",
      "[[ -1.24767834e-03   2.06178448e-04  -8.42943141e-04]\n",
      " [ -2.36315560e-03  -3.43271436e-06  -1.45260695e-03]\n",
      " [ -1.02679193e-04   1.91382406e-05  -7.01642248e-05]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Simple RNN for adding 2 numbers in binary\n",
    "'''\n",
    "weights_hidden = weights_hidden1.copy()\n",
    "weights_previous_hidden = weights_previous_hidden1.copy()\n",
    "weights_output = weights_output1.copy()\n",
    "\n",
    "\n",
    "def sigmoid(x): return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "\n",
    "batch_error = 0\n",
    "\n",
    "# online learning: network gets updated with each sample on the way\n",
    "for i in range(n_samples):\n",
    "\n",
    "    # generate 2 random numbers and their sum\n",
    "    input_1, input_2 = samples[i]\n",
    "    true_output = input_1 + input_2\n",
    " \n",
    "    print 100*'#' + \" sample {} \".format(i)\n",
    "    print \" Training sample: {0} + {1} = {2}\".format(input_1, input_2, true_output)\n",
    "\n",
    "    batch_error = 0\n",
    "    \n",
    "    # calculate the binaries\n",
    "    input_1_binary, input_2_binary, true_output_binary = [int(x) for x in np.binary_repr(input_1, n_bit)], [int(x) for x\n",
    "                                in np.binary_repr(input_2, n_bit)], [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "\n",
    "    # we'll append the outputs at each layer on the way..\n",
    "    hidden_layer_output_seq = []\n",
    "    hidden_layer_output_seq.append(np.zeros((1,hidden_size)))\n",
    "    output_layer_output_seq = []\n",
    "\n",
    "    # forward pass of the bit sequence through the network and accumulating the errors at each bit position\n",
    "    for bit_idx in range(n_bit - 1, -1, -1):\n",
    "        \n",
    "        input_bits = np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]])\n",
    "        hidden_layer_outputs = sigmoid(np.dot(input_bits, weights_hidden) + np.dot(hidden_layer_output_seq[-1], weights_previous_hidden))\n",
    "        #print \"hidden layer outputs\"\n",
    "        #print hidden_layer_outputs\n",
    "        #print \"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\"\n",
    "        #print input_bits\n",
    "        #print weights_hidden\n",
    "        #print hidden_layer_output_seq[-1]\n",
    "        #print weights_previous_hidden\n",
    "        output_layer_output = sigmoid(np.dot(hidden_layer_outputs, weights_output))\n",
    "        #print \"output layer outputs\"\n",
    "        #print output_layer_output\n",
    "        #print \"-------------------------\"\n",
    "\n",
    "        hidden_layer_output_seq.append(copy.deepcopy(hidden_layer_outputs))\n",
    "        output_layer_output_seq.append(copy.deepcopy(output_layer_output))\n",
    "        \n",
    "    \n",
    "    previous_hidden_layer_error_weighted_derivative = np.zeros((1,hidden_size))\n",
    "    # append one more zero array for going backwards\n",
    "    \n",
    "    # sum of the derivative of the outputs at the corresponding layers weighted by the errors, for each pair of input bits\n",
    "    sum_hidden_layer_updates = np.zeros_like(weights_hidden)\n",
    "    sum_previous_hidden_layer_updates = np.zeros_like(weights_previous_hidden)\n",
    "    sum_output_layer_updates = np.zeros_like(weights_output)\n",
    "\n",
    "    # rolling back from the last bit to the first\n",
    "    hidden_layer_output_seq.reverse()\n",
    "    #hidden_layer_output_seq = hidden_layer_output_seq[1:]\n",
    "    output_layer_output_seq.reverse()\n",
    "    #print hidden_layer_output_seq\n",
    "    for bit_idx in range(n_bit):\n",
    "               \n",
    "        # take output error at this position -> size(output_dim)\n",
    "        output_error = np.array([true_output_binary[bit_idx]]) - output_layer_output_seq[bit_idx]\n",
    "        print \"output layer: {}\".format(output_layer_output_seq[bit_idx])\n",
    "        # calculate output derivative weighted by the output errors -> size(output_dim)\n",
    "        output_error_weighted_derivative = sigmoid_derivative(output_layer_output_seq[bit_idx])* output_error\n",
    "        \n",
    "        # sum the output_error_weighted_derivative for each element in the sequence weighted by the size of inputs int this layer -> (hidden_size, output_dim)\n",
    "        sum_output_layer_updates += np.dot(hidden_layer_output_seq[bit_idx].T, output_error_weighted_derivative)\n",
    "        #rint \"out delta\"\n",
    "        #rint np.dot(hidden_layer_output_seq[bit_idx].T, output_error_weighted_derivative).T\n",
    "        # calculate hidden error as coming from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "        #  -> (output_dim)* (hidden_size, output_dim) + (hidden_size)*(hidden_size, hidden_size) = (hidden_size)\n",
    "        hidden_error = np.dot(output_error_weighted_derivative, weights_output.T) + np.dot(previous_hidden_layer_error_weighted_derivative, \n",
    "                                                                                           weights_previous_hidden)\n",
    "\n",
    "        # calculate hidden outputs derivatives weighted by hidden errors ->(hidden_size) * (hidden_size) = (hidden_size)\n",
    "        #print np.dot(output_error_weighted_derivative, weights_output.T)\n",
    "        #print np.dot(previous_hidden_layer_error_weighted_derivative, weights_previous_hidden)\n",
    "        hidden_error_weighted_derivative = sigmoid_derivative(hidden_layer_output_seq[bit_idx])* hidden_error\n",
    "        # print hidden_error_weighted_derivative.shape\n",
    "        \n",
    "        # sum the output_error_weighted_derivative for each element in the sequence, weighted by the size of the inputs -> (input_dim, hidden_size)\n",
    "        sum_hidden_layer_updates += np.dot(np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]]).T, hidden_error_weighted_derivative)\n",
    "        #rint \"hidden delta\"\n",
    "        #rint np.dot(np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]]).T, hidden_error_weighted_derivative)\n",
    "        # sum the hidden_error_weighted_derivative for each element in the sequence, weighted by the size of the inputs -> (hidden_size, hidden_size)\n",
    "        sum_previous_hidden_layer_updates += np.dot(hidden_layer_output_seq[bit_idx + 1].T, hidden_error_weighted_derivative)\n",
    "        #print \"prev hidden output\"\n",
    "        #print hidden_layer_output_seq[bit_idx + 1]\n",
    "        #print \"prev hidden delta\"\n",
    "        #print np.dot(hidden_layer_output_seq[bit_idx + 1].T, hidden_error_weighted_derivative)\n",
    "        # propagating the hidden layer error back to\n",
    "        previous_hidden_layer_error_weighted_derivative = hidden_error_weighted_derivative\n",
    "        \n",
    "        # just accumulating error for printing\n",
    "        batch_error += abs(output_error[0])\n",
    "        #print \"*\"*120\n",
    "    # updating weights for this sample\n",
    "    print (sum_output_layer_updates * learning_rate).T\n",
    "    print (sum_hidden_layer_updates * learning_rate)\n",
    "    print (sum_previous_hidden_layer_updates * learning_rate)\n",
    "    weights_hidden += (sum_hidden_layer_updates * learning_rate)\n",
    "    weights_previous_hidden += (sum_previous_hidden_layer_updates * learning_rate)\n",
    "    weights_output += (sum_output_layer_updates * learning_rate)\n",
    "    \n",
    "    errors = np.array(true_output_binary) - np.array([x.tolist()[0][0] for x in output_layer_output_seq])\n",
    "    batch_error += sum([abs(x) for x in errors])/n_bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training sample: 1 + 1 = 2\n",
      "************************************************************************************************************************\n",
      "<function logloss at 0x7f7985a8e938>\n",
      " Training sample: 1 + 3 = 4\n",
      "************************************************************************************************************************\n",
      "<function logloss at 0x7f7985a8e938>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from numpy import ones, zeros, zeros_like, log, clip\n",
    "\n",
    "######################################### THE DATA #########################################\n",
    "\n",
    "\n",
    "n_samples = 2\n",
    "print_every = 1\n",
    "n_bit = 3\n",
    "largest_input_number = pow(2, n_bit)/2\n",
    "\n",
    "def generate_random_sample((input_1, input_2)):\n",
    "    # generate 2 random numbers and their sum\n",
    "    \n",
    "    true_output = input_1 + input_2\n",
    "\n",
    "    print \" Training sample: {0} + {1} = {2}\".format(input_1, input_2, true_output)\n",
    "\n",
    "    # calculate the binaries\n",
    "    input_1_binary = [int(x) for x in np.binary_repr(input_1, n_bit)]\n",
    "    input_2_binary = [int(x) for x in np.binary_repr(input_2, n_bit)]\n",
    "    true_output_binary = [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "\n",
    "    return list(reversed(input_1_binary)), list(reversed(input_2_binary)), list(reversed(true_output_binary))\n",
    "\n",
    "############################################# THE RNN #############################################\n",
    "\n",
    "# RNN params\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "recursive_size = hidden_size\n",
    "\n",
    "\n",
    "# RNN weights\n",
    "# simple RNN with one recurent hidden layer and one output layer\n",
    "\n",
    "\n",
    "weights = { # hidden layer weights\n",
    "           \"recursive\": weights_hidden1.copy(),\n",
    "           \"previous_recursive\": weights_previous_hidden1.copy(),\n",
    "           \"recursive_bias\": zeros((1, recursive_size)),\n",
    "            # output layer weights\n",
    "           \"dense\":weights_output1.copy(),\n",
    "           \"dense_bias\": zeros((1,output_dim)),\n",
    "            # the associated metrics with this set of weights' values\n",
    "            \"log_loss\":0\n",
    "          }\n",
    "\n",
    "# RNN Functions\n",
    "\n",
    "# first thing first - what do we measure?\n",
    "def logloss(target, predicted, eps=1e-15): return log(1-clip(predicted, eps, 1-eps))*(target-1) - log(clip(predicted, eps, 1-eps))*target\n",
    "# compute the loss for a sequence of target and predicted values\n",
    "def compute_loss_seq(targets, predicted):\n",
    "    assert len(targets) == len(predicted)\n",
    "    return np.mean([logloss(x[0], x[1]) for x in np.stack([targets, predicted], 1)])\n",
    "\n",
    "# util math functions\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "\n",
    "\n",
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "\n",
    "def feed_forward_recursive_layer(inputs, weights):  # input_data, previous_recursive_layer_output):\n",
    "\n",
    "    raw_outputs = np.dot(inputs[\"from_previous\"], weights[\"recursive\"]) + np.dot(\n",
    "        inputs[\"from_recursive\"], weights[\"previous_recursive\"]) + weights[\"recursive_bias\"]\n",
    "    \n",
    "    #print \"**************************\"\n",
    "    #print inputs[\"from_previous\"]\n",
    "    #print weights[\"recursive\"]\n",
    "    return {\"activation\": sigmoid(raw_outputs)}\n",
    "\n",
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "def feed_forward_dense_layer(inputs, weights):\n",
    "    #print \"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\"\n",
    "    #print inputs[\"from_previous\"]\n",
    "    #print weights[\"dense\"]\n",
    "    raw_output = np.dot(inputs[\"from_previous\"], weights[\"dense\"]) + weights[\"dense_bias\"]\n",
    "\n",
    "    return {\"activation\": sigmoid(raw_output)}\n",
    "\n",
    "# feed forward one sample unit through all layers\n",
    "def feed_forward_network(inputs, weights):\n",
    "    recursive_layer_outputs = feed_forward_recursive_layer(inputs, weights)\n",
    "    dense_layer_outputs = feed_forward_dense_layer({\"from_previous\": recursive_layer_outputs[\"activation\"]}, weights)\n",
    "\n",
    "    return {\"from_dense\": dense_layer_outputs, \"from_recursive\": recursive_layer_outputs}\n",
    "\n",
    "\n",
    "# feeds forward a sequence of samples..\n",
    "def feed_forward_network_sequence(inputs_seq, weights):\n",
    "    all_samples_output_seq = [{\"from_recursive\": {\"activation\":zeros((1, recursive_size))}}]\n",
    "    for input_unit in inputs_seq:\n",
    "        input_unit[\"from_recursive\"] = all_samples_output_seq[-1][\"from_recursive\"][\"activation\"]\n",
    "        all_samples_output_seq.append(feed_forward_network(input_unit, weights))\n",
    "\n",
    "    return all_samples_output_seq[1:]\n",
    "\n",
    "# gets the error delta it sent to output and the layer input and returns the delta to pass down and\n",
    "# the delta to update its weights\n",
    "def backprop_dense_layer(inputs, outputs, errors, weights):\n",
    "    # delta at this layer\n",
    "    total_delta = sigmoid_derivative(outputs[\"activation\"]) * errors[\"to_output\"]  \n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta)\n",
    "\n",
    "    return {\"total_delta\": total_delta, \"input_w_delta\": input_w_delta}\n",
    "\n",
    "# backprop through time rnn layer\n",
    "# takes: its raw output, all the errors deltas sent to its successors\n",
    "# returns: the overall error delta to pass to its precedessors and the deltas to update its own weights\n",
    "def backprop_recursive_layer(inputs, outputs, errors,\n",
    "                             weights):  # error_to_output, error_to_next_recursive,  layer_raw_output):\n",
    "    \n",
    "    # calculate error as coming back from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "    error = np.dot(errors[\"to_output\"], weights[\"dense\"].T) + np.dot(errors[\"to_next_recursive\"],weights[\"previous_recursive\"])\n",
    "    #print \"errors to  output:\"\n",
    "    #print np.dot(errors[\"to_output\"], weights[\"dense\"].T)\n",
    "    #print \"errors to next recursive:\"\n",
    "    #print np.dot(errors[\"to_next_recursive\"],weights[\"previous_recursive\"])\n",
    "    \n",
    "    \n",
    "    # total delta of the layer to pass further down to previous inputing layers: error_weighted_derivative of output\n",
    "    total_delta = sigmoid_derivative(outputs[\"activation\"]) * error\n",
    "    #print \"total delta to pass down to next recursive\"\n",
    "    #print total_delta\n",
    "    # delta corresponding to input from below layer based on inputs from that layer\n",
    "    input_w_delta = np.dot(inputs[\"from_previous\"].T, total_delta)\n",
    "    # delta corresponding to input from previous hidden layer based on inputs from that layer\n",
    "    recursive_w_delta = np.dot(inputs[\"from_recursive\"].T, total_delta)\n",
    "    \n",
    "    return {\"total_delta\": total_delta, \"recursive_w_delta\": recursive_w_delta, \"input_w_delta\": input_w_delta}\n",
    "\n",
    "# back prop one sample unit through all layers\n",
    "# because it's recursive it takes possible deltas from successor samples feeded forward, just as the feed forward takes recursive\n",
    "# outputs from previous samples\n",
    "# should return/fill the updates coresponding to this sample\n",
    "def back_prop_network(inputs, all_layer_outputs, target, next_sample_deltas, weights):\n",
    "    \n",
    "    inputs_dense = {\"from_previous\": all_layer_outputs[\"from_recursive\"][\"activation\"]}\n",
    "    outputs_dense = all_layer_outputs[\"from_dense\"]\n",
    "    errors_dense = {\"to_output\": target - all_layer_outputs[\"from_dense\"][\"activation\"]}\n",
    "    dense_deltas = backprop_dense_layer(inputs_dense, outputs_dense, errors_dense, weights)\n",
    "\n",
    "    inputs_recursive = inputs\n",
    "    outputs_recursive = all_layer_outputs[\"from_recursive\"]\n",
    "    errors_recursive = {\"to_output\": dense_deltas[\"total_delta\"],\n",
    "                        \"to_next_recursive\": next_sample_deltas[\"recursive_deltas\"][\"total_delta\"]}\n",
    "    recursive_deltas = backprop_recursive_layer(inputs_recursive, outputs_recursive, errors_recursive, weights)\n",
    "    \n",
    "    '''\n",
    "    print \"inputs\"\n",
    "    print inputs\n",
    "    print \"output\"\n",
    "    \n",
    "    print \"all deltas \" + \"-\"*20\n",
    "    print dense_deltas['input_w_delta'].T\n",
    "    print recursive_deltas['input_w_delta']\n",
    "    print recursive_deltas['recursive_w_delta']\n",
    "    \n",
    "    '''\n",
    "    #print all_layer_outputs[\"from_dense\"]\n",
    "    #print \"-\"*20\n",
    "\n",
    "\n",
    "    return {\"dense_deltas\": dense_deltas, \"recursive_deltas\": recursive_deltas}\n",
    "\n",
    "# back propagates a sequence of samples - we don't pass delta from previous sequence here\n",
    "def back_prop_network_sequence(inputs_seq, outputs_seq, target_seq, weights):\n",
    "    # dense deltas are not going to be used so no init is needed\n",
    "    init_recursive_deltas = {\"total_delta\": zeros((1, recursive_size)),\n",
    "                             \"recursive_w_delta\": zeros_like(weights[\"previous_recursive\"]),\n",
    "                             \"input_w_delta\": zeros_like(weights[\"recursive\"])}\n",
    "    init_dense_deltas = {\"total_delta\": 0, \"input_w_delta\": zeros_like(weights[\"dense\"])}\n",
    "    all_deltas_seq = [{\"dense_deltas\": init_dense_deltas, \"recursive_deltas\": init_recursive_deltas}]\n",
    "\n",
    "    for i in range(1, len(inputs_seq)+1):\n",
    "        #print \"output: {}\".format(outputs_seq[-i][\"from_dense\"])\n",
    "        deltas = back_prop_network(inputs_seq[-i], outputs_seq[-i], target_seq[-i], all_deltas_seq[-1], weights)\n",
    "        all_deltas_seq.append(deltas.copy())\n",
    "\n",
    "\n",
    "    # compute loss for the whole sequence\n",
    "    weights[\"log_loss\"] += compute_loss_seq(target_seq, [x['from_dense']['activation'][0][0] for x in outputs_seq])\n",
    "\n",
    "    return all_deltas_seq[1:]\n",
    "\n",
    "\n",
    "# update weights with a seq  of deltas coresponding to a sequence of inputs\n",
    "# also compute the log loss of the previous set of weights\n",
    "def update_network_weights(all_deltas_seq, weights):\n",
    "    for all_deltas in all_deltas_seq:\n",
    "        \n",
    "        #print learning_rate * all_deltas[\"recursive_deltas\"][\"recursive_w_delta\"] \n",
    "        #print learning_rate * np.clip(all_deltas[\"recursive_deltas\"][\"input_w_delta\"], -100, 100)\n",
    "        #print learning_rate * np.clip(all_deltas[\"recursive_deltas\"][\"recursive_w_delta\"], -100, 100)\n",
    "\n",
    "        weights[\"recursive\"] += learning_rate * np.clip(all_deltas[\"recursive_deltas\"][\"input_w_delta\"], -100, 100)\n",
    "\n",
    "        #weights[\"recursive_bias\"] += learning_rate * np.clip(all_deltas[\"recursive_deltas\"][\"total_delta\"], -100, 100)\n",
    "        weights[\"dense\"] += learning_rate * np.clip(all_deltas[\"dense_deltas\"][\"input_w_delta\"], -100, 100)\n",
    "        #weights[\"dense_bias\"] -= learning_rate * np.clip(all_deltas[\"dense_deltas\"][\"total_delta\"], -10, 10)\n",
    "        weights[\"previous_recursive\"] += learning_rate * all_deltas[\"recursive_deltas\"][\"recursive_w_delta\"] \n",
    "\n",
    "\n",
    "def train_net(weights):\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "\n",
    "        input_1_binary, input_2_binary, target_binary = generate_random_sample(samples[i])\n",
    "\n",
    "\n",
    "        input_seq = [{\"from_previous\": np.array([x]), \"from_recursive\": zeros((1, recursive_size))} for x in\n",
    "                     zip(input_1_binary, input_2_binary)]\n",
    "\n",
    "        net_outputs = feed_forward_network_sequence(input_seq, weights)\n",
    "        #for k in range(len(net_outputs)):\n",
    "            #print net_outputs[k]['from_recursive']\n",
    "            #print net_outputs[k]['from_dense']\n",
    "\n",
    "        net_deltas = back_prop_network_sequence(input_seq, net_outputs, target_binary, weights)\n",
    "        #for delta net_deltas\n",
    "        update_network_weights( net_deltas, weights)\n",
    "            \n",
    "\n",
    "        if i % print_every ==0:\n",
    "            print \"*\"*120\n",
    "            print logloss\n",
    "    \n",
    "train_net(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# change 1 - plus  delta updates\n",
    "# change 2 - no bias\n",
    "# change 3 - no clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
