{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simple RNN for adding 2 numbers in binary\n",
    "\n",
    "forward forward..\n",
    "backward - we want to iteratively change the weights starting from random positions \n",
    "to minimize error we change the weights of each layer in the directions of the derivatives of the output  of that layer\n",
    "(note that the overall error function to optimize is formed by all layer functions but as we go back we no longer care  \n",
    "about the functions in front and their weights..)\n",
    "we want the change to be proportional to the size of the error and also the size of the input - so we weight the derivatives \n",
    "by the errors deltas and inputs\n",
    "when passing error delta back to previous layer - we multiply current error weighted derivative by the weights to see how much\n",
    "of the erro corresponds to each of the previous layer outputs\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "np.random.seed(0)\n",
    "from numpy import ones, zeros\n",
    "\n",
    "# the data generating params\n",
    "n_samples = 1#5000\n",
    "n_bit = 8\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "\n",
    "# RNN params\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "recursive_size = 16\n",
    "learning_rate = .1\n",
    "\n",
    "#### done with constants\n",
    "\n",
    "def generate_random_sample():\n",
    "    # generate 2 random numbers and their sum\n",
    "    input_1, input_2 = np.random.randint(0, largest_input_number), np.random.randint(0, largest_input_number)\n",
    "    true_output = input_1 + input_2\n",
    "\n",
    "    # calculate the binaries\n",
    "    input_1_binary = [int(x) for x in np.binary_repr(input_1, n_bit)]\n",
    "    input_2_binary = [int(x) for x in np.binary_repr (input_2, n_bit)]\n",
    "    true_output_binary = [int(x) for x in np.binary_repr(true_output, n_bit)]\n",
    "    \n",
    "    return input_1_binary, input_2_binary, true_output_binary\n",
    "\n",
    "\n",
    "\n",
    "# util math functions\n",
    "def sigmoid(x): return (1 / (1 + np.exp(-x)))\n",
    "def sigmoid_derivative(x): return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "# simple RNN with one recurent hidden layer and one output layer\n",
    "\n",
    "# hidden layer weights\n",
    "w_recursive = np.random.standard_normal(size=(input_dim, recursive_size))\n",
    "w_previous_recursive = np.random.standard_normal(size=(recursive_size, recursive_size))\n",
    "# output layer weights\n",
    "w_dense = np.random.standard_normal(size=(recursive_size, output_dim))\n",
    "\n",
    "# training containers to store the the train input and output values at each layer\n",
    "\n",
    "recursive_raw_output_seq = [] # values before activation\n",
    "recursive_output_seq = [] # values after activation\n",
    "dense_raw_output_seq = [] # values before activation\n",
    "dense_output_seq = [] # values after activation\n",
    "\n",
    "# containers for error deltas for updating weights\n",
    "deltas_recursive = []\n",
    "deltas_previous_recursive = []\n",
    "deltas_dense = []\n",
    "\n",
    "# init - because it's recursive\n",
    "recursive_output_seq.append(np.zeros((1,recursive_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 0, 1], [0, 0, 0], [0, 0, 1])\n",
      "[[ 0.5         0.73105858  0.88079708]]\n",
      "[[ 0  0 -2]]\n"
     ]
    }
   ],
   "source": [
    "# test functions - not asserting correct results - just making sure they run with correct dimensions\n",
    "\n",
    "# set test constants\n",
    "\n",
    "n_bit = 3\n",
    "largest_input_number = pow(2, n_bit) / 2\n",
    "recursive_size = 3\n",
    "sample_data= np.array([[0,1]])\n",
    "error = 1\n",
    "\n",
    "# init test weights to 1 for simple test of correct values\n",
    "w_recursive = np.ones((input_dim, recursive_size))\n",
    "w_previous_recursive = np.ones((recursive_size, recursive_size))\n",
    "w_dense = np.ones((recursive_size, output_dim))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.,  4.,  4.]])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "def feed_forward_recursive_layer(input_data, previous_recursive_layer_output):\n",
    "    \n",
    "    raw_outputs = np.dot(input_data, w_recursive) + np.dot(previous_recursive_layer_output, w_previous_recursive)\n",
    "    recursive_raw_output_seq.append(raw_outputs)\n",
    "    recursive_output_seq.append(sigmoid(raw_outputs))\n",
    "\n",
    "#test\n",
    "recursive_raw_output_seq = []\n",
    "feed_forward_recursive_layer(sample_data, np.ones((1,recursive_size)))\n",
    "print recursive_raw_output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 3.]])]\n"
     ]
    }
   ],
   "source": [
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "def feed_forward_dense_layer(input_data):\n",
    "    \n",
    "    raw_output = np.dot(input_data, w_dense)\n",
    "    dense_raw_output_seq.append(raw_output)\n",
    "    dense_output_seq.append(sigmoid(raw_output))\n",
    "\n",
    "dense_raw_output_seq = []\n",
    "feed_forward_dense_layer(np.ones((1,recursive_size))) \n",
    "print dense_raw_output_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gets the error delta it sent to output and the layer input and returns the delta to pass down and \n",
    "# the delta to update its weights\n",
    "def backprop_dense_layer(error):\n",
    "    \n",
    "    error_weighted_derivative = 1* error # being the output dense layer, derivative = 1\n",
    "    #input_and_error_weighted_derivative = np.dot(layer_input, error_weighted_derivative)\n",
    "    \n",
    "    deltas_dense.append(error_weighted_derivative)\n",
    "\n",
    "deltas_dense = []\n",
    "backprop_dense_layer(error) \n",
    "print deltas_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.75,  0.75,  0.75]]), array([[ 0.75,  0.75,  0.75]])]\n"
     ]
    }
   ],
   "source": [
    "# backprop through time rnn layer \n",
    "# takes: its raw output, all the errors deltas sent to its successors\n",
    "# returns: the overall error delta to pass to its precedessors\n",
    "def backprop_recursive_layer(error_to_output, error_to_next_recursive,  layer_raw_output):\n",
    "    \n",
    "    # calculate error as coming back from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "    error = np.dot(error_to_output, w_dense) + np.dot(error_to_next_recursive, w_previous_recursive)\n",
    "    # total delta of the layer to pass further down to previous inputing layers: error_weighted_derivative\n",
    "    error_weighted_derivative = sigmoid_derivative(layer_raw_output)* error \n",
    "    deltas_recursive.append(error_weighted_derivative)\n",
    "    # delta corresponding to input from below layer based on inputs from that layer\n",
    "    #input_and_error_weighted_derivative = np.dot(layer_input.T, error_weighted_derivative) \n",
    "    # delta corresponding to input from previous hidden layer based on inputs from that layer\n",
    "    #previous_hidden_input_and_error_weighted_derivative = np.dot(layer_previous_hidden_input.T, error_weighted_derivative)\n",
    "deltas_recursive = []\n",
    "# assume there was no error sent to next hidden layer\n",
    "backprop_recursive_layer(ones((1,recursive_size)),  zeros((1,recursive_size)), ones((1,recursive_size))/2)\n",
    "# assume there was no error sent to next layer (the output dense layer)\n",
    "backprop_recursive_layer(zeros((1,recursive_size)),  ones((1,recursive_size)), ones((1,recursive_size))/2)\n",
    "\n",
    "print deltas_recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.,  1.,  1.]])]\n",
      "[array([[ 2.19317574]])]\n"
     ]
    }
   ],
   "source": [
    "# feed forward one sample unit through all layers\n",
    "def feed_forward_network(input_sample):\n",
    "    \n",
    "    feed_forward_recursive_layer(input_sample, recursive_output_seq[-1])\n",
    "    feed_forward_dense_layer(recursive_output_seq[-1])\n",
    "\n",
    "\n",
    "recursive_raw_output_seq = []\n",
    "recursive_output_seq = []\n",
    "dense_raw_output_seq = []\n",
    "# init - because it's recursive\n",
    "recursive_output_seq.append(np.zeros((1,recursive_size)))\n",
    "\n",
    "feed_forward_network(sample_data)\n",
    "\n",
    "print recursive_raw_output_seq\n",
    "print dense_raw_output_seq \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# back prop one sample unit through all layers\n",
    "# containers are full from feeding forward each element and backward up to this point\n",
    "# it assumed that all elems in the containers before this have been poped out going backward\n",
    "# should return/fill the updates coresponding to this sample\n",
    "def back_prop_network(input_sample, dense_layer_output,  ):\n",
    "    \n",
    "    out_error = correct_output - layer outputs\n",
    "    backprop_dense_layer(out_error) # delta dense has been updated \n",
    "    backprop_recursive_layer(deltas_dense[-1], deltas_recursive[-1],  layer_raw_output) # delta recursive has been updated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feed forward one sample unit through all layers\n",
    "def feed_forward_network_sequence(input_sequence):\n",
    "    \n",
    "    for seq_elem in input_sequence:\n",
    "        feed_forward_network(input_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def back_prop_network_seq(input_sequence, layer_raw_output, out_error):\n",
    "    \n",
    "    backprop_dense_layer(out_error)\n",
    "    backprop_recursive_layer(deltas_dense[-1], deltas_recursive[-1],  layer_raw_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-123-95d0901842bf>, line 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-123-95d0901842bf>\"\u001b[0;36m, line \u001b[0;32m64\u001b[0m\n\u001b[0;31m    backprop_rnn_layer(layer_input, error_to_output, layer_previous_hidden_input, error_to_next_hidden,  layer_raw_output):\u001b[0m\n\u001b[0m                                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# back prop one sample unit through all layers\n",
    "# it assumed that all elems in the containers before this have been poped out going backward\n",
    "def back_prop_network(input_sample, out_error):\n",
    "    \n",
    "    # pop out next elements from the stack of outputs sequence at the  hidden layer\n",
    "    # last output was inputed into the dense layer\n",
    "    # last last output was inputed into both the dense layer and the next hidden layer\n",
    "    dense_layer_input, hidden_layer_input = hidden_layer_output_seq[-1], hidden_layer_output_seq[-2]\n",
    "    hidden_layer_output_seq = hidden_layer_output_seq[:-1]\n",
    "    \n",
    "    # backprop the dense layer and store the update for the weights\n",
    "    pass_next_deltas, own_weights_deltas = backprop_dense_layer(out_error, input_sample)\n",
    "    deltas_output.append(own_weights_deltas)\n",
    "    \n",
    "    # backprop the hidden layer and store the update for the weights\n",
    "    backprop_rnn_layer(layer_input, error_to_output, layer_previous_hidden_input, error_to_next_hidden,  layer_raw_output):\n",
    "        \n",
    "    pass_next_deltas, input_weights_deltas, prev_hidden_input_weights_deltas = backprop_rnn_layer(\n",
    "        input_sample, pass_next_deltas, hidden_layer_input, hidden_err_weighted_deriv, hidden_layer_raw_output_seq[bit_idx].T)\n",
    "    \n",
    "\n",
    "    batch_error += abs(out_error[0])\n",
    "    \n",
    "def train_RNN():\n",
    "    \n",
    "    # online learning: network gets updated with each sample on the way\n",
    "    for i in range(n_samples):\n",
    "        \n",
    "        \n",
    "        input_1_binary, input_2_binary, true_output_binary = generate_random_sample()\n",
    "        \n",
    "        # delta updates for this sample\n",
    "        sum_hidden_layer_deltas = np.zeros_like(weights_hidden)\n",
    "        sum_previous_hidden_layer_deltas = np.zeros_like(weights_previous_hidden)\n",
    "        sum_output_layer_deltas = np.zeros_like(weights_output)\n",
    "        \n",
    "        # storing outputs in lists\n",
    "        hidden_layer_raw_output_seq = []\n",
    "        hidden_layer_output_seq = []\n",
    "        output_layer_raw_output_seq = []\n",
    "        output_layer_output_seq = []\n",
    "        # init\n",
    "        hidden_layer_output_seq.append(np.zeros((1,hidden_size)))\n",
    "        \n",
    "        # forward pass of the bit sequence through the network \n",
    "        for bit_idx in range(n_bit - 1, -1, -1):\n",
    "            input_bits = np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]])\n",
    "            \n",
    "            hidden_layer_raw_outputs, hidden_layer_outputs = feed_forward_rnn_layer(input_bits, hidden_layer_output_seq[-1])\n",
    "            hidden_layer_raw_output_seq.append(hidden_layer_raw_outputs)\n",
    "            hidden_layer_output_seq.append(hidden_layer_outputs)\n",
    "            \n",
    "            output_layer_raw_output, output_layer_output = feed_forward_dense_layer(hidden_layer_outputs)\n",
    "            output_layer_raw_output_seq.append(output_layer_raw_outputs)\n",
    "            output_layer_output_seq.append(output_layer_outputs)\n",
    "            \n",
    "        # backward pass of the bit sequence through the network \n",
    "        # first init output error of recurrent layer\n",
    "        hidden_error_weighted_derivative = np.zeros((1,hidden_size))\n",
    "        for bit_idx in range(n_bit - 1, -1, -1):\n",
    "            \n",
    "            input_bits = np.array([[input_1_binary[bit_idx], input_2_binary[bit_idx]]])\n",
    "            out_error = np.array([true_output_binary[bit_idx]]) - output_layer_output_seq[bit_idx]\n",
    "            \n",
    "            error_weighted_deriv_out, input_and_error_weighted_deriv_out = \n",
    "                backprop_dense_layer(out_error, hidden_layer_output_seq[bit_idx].T)\n",
    "            sum_output_layer_deltas += input_and_error_weighted_deriv_out\n",
    "\n",
    "            hidden_err_weighted_deriv, hidden_input_and_err_weighted_deriv, previous_hidden_input_and_err_weighted_deriv = backprop_rnn_layer(\n",
    "                input_bits, error_weighted_deriv_out, hidden_layer_output_seq[bit_idx-1].T, hidden_err_weighted_deriv, hidden_layer_raw_output_seq[bit_idx].T)\n",
    "            sum_hidden_layer_deltas += hidden_input_and_err_weighted_deriv\n",
    "            sum_previous_hidden_layer_deltas += previous_hidden_input_and_err_weighted_deriv\n",
    "            \n",
    "            batch_error += abs(out_error[0])\n",
    " \n",
    "        # updating weights for this sample\n",
    "        weights_hidden += (sum_hidden_layer_deltas * learning_rate)\n",
    "        weights_previous_hidden += (sum_previous_hidden_layer_deltas * learning_rate)\n",
    "        weights_output += (sum_output_layer_deltas * learning_rate)\n",
    "        \n",
    "        errors = np.array(true_output_binary) - np.array([sigmoid(x.tolist()[0][0]) for x in output_layer_output_seq])\n",
    "        batch_error += sum([abs(x) for x in errors])/n_bit\n",
    "\n",
    "        if (i % 1000) == 0: \n",
    "            print 100*'#' + \" sample {} \".format(i)\n",
    "            print \" Training sample: {0} + {1} = {2}\".format(input_1, input_2, true_output)\n",
    "            #print \" Binary version: {0} + {1} = {2}\".format(input_1_binary, input_2_binary, true_output_binary)\n",
    "            result = [sigmoid(x.tolist()[0][0]) for x in output_layer_output_seq]\n",
    "            print \" Result is {}\".format( sum([pow(2,n_bit-i-1)*round(result[i]) for i in range(n_bit)]))\n",
    "            #print result\n",
    "\n",
    "            print \" Average binarry error for this batch is {}\".format(batch_error/8000)   \n",
    "            batch_error = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "\n",
    "a[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_derivative(np.ones((1,hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones((1,hidden_size))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets an input sample and recurrent input and returns all layer outputs\n",
    "def feed_forward_lstm_layer(input_data, previous_hidden_layer_output):\n",
    "    \n",
    "    hidden_layer_raw_outputs = np.dot(input_data, w_hidden) + np.dot(previous_hidden_layer_output, w_previous_hidden)\n",
    "    hidden_layer_outputs = sigmoid(hidden_layer_outputs)\n",
    "    \n",
    "    return hidden_layer_raw_outputs, hidden_layer_outputs\n",
    "\n",
    "# backprop through time rnn layer   \n",
    "def backprop_lstm_layer(layer_input, error_to_output, layer_previous_hidden_input, error_to_next_hidden,  layer_raw_output):\n",
    "    \n",
    "    # calculate error as coming back from: 1.what was sent to the output, 2.what was sent to the next hidden layer\n",
    "    error = np.dot(error_to_output, weights_output.T) + np.dot(error_to_next_hidden, weights_previous_hidden)\n",
    "    error_weighted_derivative = sigmoid_derivative(layer_raw_output)* error\n",
    "    layer_input_and_error_weighted_derivative = np.dot(layer_input, error_weighted_derivative)\n",
    "    previous_hidden_input_and_error_weighted_derivative = np.dot(layer_input, error_weighted_derivative)\n",
    "    \n",
    "    return error_weighted_derivative, layer_input_and_error_weighted_derivative, previous_hidden_input_and_error_weighted_derivative\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
